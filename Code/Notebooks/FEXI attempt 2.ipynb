{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import scipy.stats\n",
    "\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_bvalues\n",
    "from dmipy.core.modeling_framework import MultiCompartmentSphericalMeanModel\n",
    "from dmipy.signal_models import sphere_models, cylinder_models, gaussian_models\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sig_np(bf,be,tm,adc,sigma,axr,nvox):\n",
    "    \n",
    "    be_tiled = np.tile(be,(nvox,1))\n",
    "    bf_tiled = np.tile(bf,(nvox,1))\n",
    "    tm_tiled = np.tile(tm,(nvox,1))\n",
    "\n",
    "    adc_tiled = np.transpose(np.tile(adc,(np.size(tm),1)))\n",
    "    sigma_tiled = np.transpose(np.tile(sigma,(np.size(tm),1)))\n",
    "    axr_tiled = np.transpose(np.tile(axr,(np.size(tm),1)))\n",
    "\n",
    "    tm_tiled[(tm_tiled == np.min(tm_tiled)) & (bf_tiled == 0)] = np.inf\n",
    "\n",
    "    adc_prime_tiled = adc_tiled * (1 - sigma_tiled* np.exp(-tm_tiled*axr_tiled))\n",
    "    normalised_signal_tiled = np.exp(-adc_prime_tiled * be_tiled)\n",
    "    \n",
    "    return normalised_signal_tiled, adc_prime_tiled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sig_pytorch(bf, be, tm, adc_tiled, sigma_tiled, axr_tiled, batch_size):\n",
    "    \"\"\"batch_size and nvox are kind of interchangable\n",
    "    might be useful to know for testing purposes\n",
    "    \"\"\"\n",
    "    be_tiled = torch.tile(be,(batch_size,1))\n",
    "    bf_tiled = torch.tile(bf,(batch_size,1))\n",
    "    tm_tiled = torch.tile(tm,(batch_size,1))\n",
    "\n",
    "    \"\"\"\n",
    "    use this section for testing if function works as intended    \n",
    "    adc_tiled = torch.tile(adc,(tm.shape[0],1)).t()\n",
    "    sigma_tiled = torch.tile(sigma,(tm.shape[0],1)).t()\n",
    "    axr_tiled = torch.tile(axr,(tm.shape[0],1)).t()\n",
    "    \"\"\"\n",
    "\n",
    "    tm_tiled[(tm_tiled == torch.min(tm_tiled)) & (bf_tiled == 0)] = float('inf')\n",
    "    \n",
    "\n",
    "    adc_prime_tiled = adc_tiled * (1 - sigma_tiled * torch.exp(-tm_tiled * axr_tiled))\n",
    "    normalised_signal_tiled = torch.exp(-adc_prime_tiled * be_tiled)\n",
    "\n",
    "    return normalised_signal_tiled, adc_prime_tiled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 1000 # number of voxels to simulate\n",
    "\n",
    "bf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e-3   # filter b-values [ms/um2]\n",
    "be = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e-3       # encoding b-values [ms/um2]\n",
    "tm = np.array([20, 20, 20, 20, 200, 200, 400, 400], dtype=np.float32)  # mixing time [ms]\n",
    "\n",
    "\n",
    "sim_adc = np.random.uniform(0.1,3.5,nvox)               # ADC, simulated [um2/ms]\n",
    "sim_sigma = np.random.uniform(0,1,nvox)                 # sigma, simulated [a.u.]\n",
    "sim_axr = np.random.uniform(0.1,20,nvox) * 1e-3         # AXR, simulated [ms-1]\n",
    "\n",
    "sim_E_vox, sim_adc_prime = sim_sig_np(bf,be,tm,sim_adc,sim_sigma,sim_axr,nvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sim_adc1 = torch.tensor(sim_adc)\n",
    "sim_sigma1 = torch.tensor(sim_sigma)\n",
    "sim_axr1 = torch.tensor(sim_axr)\n",
    "be = torch.tensor(be)\n",
    "bf = torch.tensor(bf)\n",
    "tm = torch.tensor(tm)\n",
    "\n",
    "sim_E_vox1, sim_adc_prime1 = sim_sig_pytorch(bf,be,tm,sim_adc1,sim_sigma1,sim_axr1,nvox)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins=20\n",
    "#make 200 and voxels 100,000 to see adc prime dropoff\n",
    "fig, axs = plt.subplots(1, 3, sharey=True,figsize=(15, 5))\n",
    "axs[0].hist(sim_adc, bins=n_bins)\n",
    "axs[1].hist(sim_sigma, bins=n_bins)\n",
    "axs[2].hist(sim_axr, bins=n_bins)\n",
    "axs[0].set_title('ADC Histogram ')\n",
    "axs[0].set_xlabel('ADC Values [um^2/ms]')\n",
    "axs[1].set_title('Sigma Histogram')\n",
    "axs[1].set_xlabel('Sigma Values [arbitrary units]')\n",
    "axs[2].set_title('AXR Histogram')\n",
    "axs[2].set_xlabel('AXR Values [ms-1]');\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3,figsize=(15, 5))\n",
    "axs[0].hist(sim_E_vox.flatten(), bins=n_bins)\n",
    "axs[0].set_title('Signal Histogram ')\n",
    "axs[0].set_xlabel('Signal Values [units?]')\n",
    "axs[1].hist(sim_E_vox[sim_E_vox != 1].flatten(), bins=n_bins)\n",
    "axs[1].set_title('Signal Histogram with ones removed')\n",
    "axs[1].set_xlabel('Signal Values [units?]')\n",
    "axs[2].hist(sim_adc_prime.flatten(), bins=n_bins)\n",
    "axs[2].set_title('ADC prime Histogram')\n",
    "axs[2].set_xlabel('ADC prime Values [units?]');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting b-value against normalised signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'be' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_41518/2049763896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextcoords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"offset points\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxytext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'go-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_E_vox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextcoords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"offset points\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxytext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'be' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([be[0], be[1]], [sim_E_vox[0,0], sim_E_vox[0,1]], 'bo-')\n",
    "plt.annotate(tm[0], (be[1], sim_E_vox[0, 1]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[2], be[3]], [sim_E_vox[0,2], sim_E_vox[0,3]], 'go-')\n",
    "plt.annotate(tm[2], (be[3], sim_E_vox[0, 3]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[4], be[5]], [sim_E_vox[0,4], sim_E_vox[0,5]], 'ko-')\n",
    "plt.annotate(tm[4], (be[5], sim_E_vox[0, 5]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[6], be[7]], [sim_E_vox[0,6], sim_E_vox[0,7]], 'mo-')\n",
    "plt.annotate(tm[6], (be[7], sim_E_vox[0, 7]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "\n",
    "#plt.title('Scatter plot with 4 lines')\n",
    "plt.xlabel('Encoding block b-value [m2/s]')\n",
    "#are units correct\n",
    "plt.ylabel('Normalised Signal (sum of the magnetisations)')\n",
    "#unit?\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox1 = 15\n",
    "bf1 = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e-3   # filter b-values [ms/um2]\n",
    "be1 = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e-3       # encoding b-values [ms/um2]\n",
    "tm1 = np.array([20, 20, 20, 20, 200, 200, 400, 400], dtype=np.float32)  # mixing time [ms]\n",
    "sim_adc1 = np.random.uniform(0.1,3.5,nvox1)               # ADC, simulated [um2/ms]\n",
    "sim_sigma1 = np.random.uniform(0,1,nvox1)                 # sigma, simulated [a.u.]\n",
    "sim_axr1 = np.random.uniform(0.1,20,nvox1) * 1e-3         # AXR, simulated [ms-1]\n",
    "sim_E_vox1, sim_adc_prime1 = sim_sig_np(bf1,be1,tm1,sim_adc1,sim_sigma1,sim_axr1,nvox1)\n",
    "\n",
    "def sse_axr_prime(variables_to_optimize, tm1, be1, smeas1):\n",
    "\n",
    "    \"\"\"Function arguments are broken\n",
    "    Need to read up how scipy.optimize.minimize uses them\n",
    "    I think it is working now. \"\"\"\n",
    "    #calculate axr prime for 1 voxel, for all 8 image volumes, from known adc, sigma, axr and tm\n",
    "    #calculate axr prime from be values, S(tm, b1) and S(tm, b2)\n",
    "    #find sse difference between each of 8 values then sum\n",
    "    sim_adc1, sim_axr1, sim_sigma1 = variables_to_optimize\n",
    "    \n",
    "    adc1 = np.transpose(np.tile(sim_adc1, (8,1)))\n",
    "    sigma1 = np.transpose(np.tile(sim_sigma1, (8,1)))\n",
    "    axr1 = np.transpose(np.tile(sim_axr1, (8,1)))\n",
    "\n",
    "    #function arguement issue breaks the next line, but that is proper way to do it\n",
    "    adc1 = np.transpose(np.tile(sim_adc1, (np.size(tm1),1)))\n",
    "    sigma1 = np.transpose(np.tile(sim_sigma1, (np.size(tm1),1)))\n",
    "    axr1 = np.transpose(np.tile(sim_axr1, (np.size(tm1),1)))\n",
    "\n",
    "    \"\"\"shouldn't be nvox\"\"\"\n",
    "    be1 = np.tile(be1,(nvox1,1))\n",
    "    #bf1 = np.tile(bf1,(nvox1,1))\n",
    "    tm1 = np.tile(tm1,(nvox1,1))\n",
    "\n",
    "    #calculate axr prime from known adc, sigma, axr and tm\n",
    "    adc_tm_fit1 = adc1 * (1 - sigma1 * np.exp(-tm1*axr1))\n",
    "\n",
    "    #calculate axr prime from be values, S(tm, b1) and S(tm, b2)\n",
    "    \"\"\"check order of be and smeas is correct\"\"\"\n",
    "    adc_tm_calc1 = -1 / (0 - 250) * np.log(smeas1)\n",
    "\n",
    "    #calculate difference for all 8 image volumes then sum\n",
    "    sse = np.sum((adc_tm_calc1 - adc_tm_fit1) ** 2)\n",
    "\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimized values of x, y, z: [0.1 1.  0.1]\n",
      "Minimum value of the function: 1.2132882393112854\n"
     ]
    }
   ],
   "source": [
    "#basic minimization function\n",
    "\n",
    "# Define the bounds for x, y, z\n",
    "bounds = [(0.1, 3.5), (1, 1), (0.1, 20)]  # Bounds for ADC, sigma, AXR\n",
    "# Initial guess\n",
    "initial_guess = [1.1, .15, 3.5 * 1e-3 ]  # Starting values for ADC , sigma, AXR\n",
    "\n",
    "additional_args = (tm1, be1, sim_E_vox1) \n",
    "# Minimize the function using L-BFGS-B method with bounds\n",
    "result = scipy.optimize.minimize(sse_axr_prime, initial_guess, args=additional_args, bounds=bounds, method='L-BFGS-B')\n",
    "\n",
    "# Print the result\n",
    "print(\"Minimized values of x, y, z:\", result.x)\n",
    "\n",
    "print(\"Minimum value of the function:\", result.fun)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # this is the neural network\n",
    "    #defining the init and foward pass functions. \n",
    "\n",
    "    def __init__(self,be,bf,tm,nvox,nparams,batch_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.be = be\n",
    "        self.bf = bf\n",
    "        self.tm = tm\n",
    "        self.nvox = nvox\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #defining the layers that we want. \n",
    "        # 3 layers with no. of be nodes. \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.layers.extend([nn.Linear(len(be), len(be)), nn.PReLU()])\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html\n",
    "        self.encoder = nn.Sequential(*self.layers, nn.Linear(len(be), nparams))\n",
    "\n",
    "    def forward(self, E_vox):\n",
    "\n",
    "        params = torch.nn.functional.softplus(self.encoder(E_vox))\n",
    "        #running a forward pass through the network\n",
    "\n",
    "        #SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive\n",
    "        #params contains batch_size x nparams outputs, so each row is adc, sigma and axr.\n",
    "\n",
    "        #unsqueeze adds an additional dimension. \n",
    "        #parameter constraints from Elizabeth matlab \n",
    "\n",
    "        adc = torch.tile(torch.clamp(params[:, 0].unsqueeze(1), min=0.1, max=3.5), (1, tm.shape[0]))\n",
    "        sigma = torch.tile(torch.clamp(params[:, 1].unsqueeze(1), min=0, max=1), (1, tm.shape[0]))\n",
    "        axr = torch.tile(torch.clamp(params[:, 2].unsqueeze(1), min=0.1e-3, max=20e-3), (1, tm.shape[0]))\n",
    "\n",
    "        E_vox,_ = sim_sig_pytorch(self.bf, self.be, self.tm, adc, sigma, axr, axr.shape[0])\n",
    "        #axr.shape[0] is either the batch size or nvox\n",
    "\n",
    "        return E_vox, adc, sigma, axr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nparams = 3\n",
    "#because of adc, sigma and axr\n",
    "\n",
    "#converting numpy arrays to pytorch tensors. \n",
    "be = torch.tensor(be)\n",
    "bf = torch.tensor(bf)\n",
    "tm = torch.tensor(tm)\n",
    "batch_size = 128\n",
    "\n",
    "#initilise network\n",
    "net = Net(be, bf, tm, nvox, nparams, batch_size)\n",
    "\n",
    "#create batch queues for data\n",
    "#// means divide and round down. \n",
    "num_batches = len(sim_E_vox) // batch_size\n",
    "\n",
    "#import the sim_E_vox array into the dataloader\n",
    "#drop_last ignores the last batch if it is the wrong size. \n",
    "#num_workers is about performance. \n",
    "\n",
    "trainloader = utils.DataLoader(torch.from_numpy(sim_E_vox.astype(np.float32)),\n",
    "                                batch_size = batch_size, \n",
    "                                shuffle = True,\n",
    "                                num_workers = 0, #was 2 previously\n",
    "                                drop_last = True)\n",
    "\n",
    "# loss function and optimizer\n",
    "#choosing which loss function to use. \n",
    "#not sure what the optmizer is\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "# best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "#can increase patience a lot, speed not an issue.\n",
    "patience = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "loss_progress = np.empty(shape=(0,)) \n",
    "for epoch in range(10000): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"epoch: {}; bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    #tqdm shows a progress bar. \n",
    "    for i, sim_E_vox_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred_E_vox, pred_adc, pred_sigma, pred_axr = net(sim_E_vox_batch)\n",
    "\n",
    "        sim_E_vox_batch64 = sim_E_vox_batch.to(torch.float64)\n",
    "        #needed so that loss comparison works\n",
    "\n",
    "        loss = criterion(pred_E_vox, sim_E_vox_batch64)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(\"loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"####################### saving good model #######################\")\n",
    "        final_model = net.state_dict()\n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "        loss_progress = np.append(loss_progress, best)\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        loss_progress = np.append(loss_progress, best)\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"done\")\n",
    "\n",
    "net.load_state_dict(final_model)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred_E_vox, final_pred_adc_repeated, final_pred_sigma_repeated, final_pred_axr_repeated = net(torch.from_numpy(sim_E_vox.astype(np.float32)))\n",
    "    # adc sigma and axr will have 8 columns which are all the same\n",
    "\n",
    "final_pred_adc = final_pred_adc_repeated[:, 0]\n",
    "final_pred_sigma = final_pred_sigma_repeated [:, 0]\n",
    "final_pred_axr = final_pred_axr_repeated[:, 0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, len(loss_progress) + 1), loss_progress, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_pred_E_vox_detached = final_pred_E_vox.detach().numpy()\n",
    "\"\"\"Was having numpy pytorch issues, so this line helps fix it a bit.\"\"\"\n",
    "\n",
    "plt.scatter(be, sim_E_vox[0,:], label='simulated')\n",
    "plt.scatter(be, final_pred_E_vox_detached[0,:], label='predicted')\n",
    "plt.legend()\n",
    "\n",
    "# plot scatter plots to analyse correlation of predicted free params against ground truth\n",
    "plt.figure()\n",
    "\n",
    "param_sim = [sim_adc, sim_sigma, sim_axr]\n",
    "param_pred = [final_pred_adc, final_pred_sigma, final_pred_axr]\n",
    "param_name = ['ADC', 'Sigma', 'AXR']\n",
    "\n",
    "rvals = []\n",
    "\n",
    "for i,_ in enumerate(param_sim):\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "    plt.scatter(param_sim[i], param_pred[i], s=2, c='navy')\n",
    "    plt.xlabel(param_name[i] + ' Ground Truth')\n",
    "    plt.ylabel(param_name[i] + ' Prediction')\n",
    "    rvals.append(scipy.stats.pearsonr(np.squeeze(param_sim[i]), np.squeeze(param_pred[i])))\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "print(rvals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530a9f706d6870d645c2a38796218763bd8210a92912827e1b5537d0d478cbf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
