{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import scipy.stats\n",
    "\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_bvalues\n",
    "from dmipy.core.modeling_framework import MultiCompartmentSphericalMeanModel\n",
    "from dmipy.signal_models import sphere_models, cylinder_models, gaussian_models\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sig_np(bf,be,tm,adc,sigma,axr,nvox):\n",
    "    \n",
    "    be_tiled = np.tile(be,(nvox,1))\n",
    "    bf_tiled = np.tile(bf,(nvox,1))\n",
    "    tm_tiled = np.tile(tm,(nvox,1))\n",
    "\n",
    "    adc_tiled = np.transpose(np.tile(adc,(np.size(tm),1)))\n",
    "    sigma_tiled = np.transpose(np.tile(sigma,(np.size(tm),1)))\n",
    "    axr_tiled = np.transpose(np.tile(axr,(np.size(tm),1)))\n",
    "\n",
    "    tm_tiled[(tm_tiled == np.min(tm_tiled)) & (bf_tiled == 0)] = np.inf\n",
    "\n",
    "    adc_prime_tiled = adc_tiled * (1 - sigma_tiled* np.exp(-tm_tiled*axr_tiled))\n",
    "    normalised_signal_tiled = np.exp(-adc_prime_tiled * be_tiled)\n",
    "    \n",
    "    return normalised_signal_tiled, adc_prime_tiled\n",
    "\n",
    "def sim_sig_np_1_vox(bf,be,tm,adc,sigma,axr):\n",
    "\n",
    "    adc_tiled = np.transpose(np.tile(adc,(np.size(tm),1)))\n",
    "    sigma_tiled = np.transpose(np.tile(sigma,(np.size(tm),1)))\n",
    "    axr_tiled = np.transpose(np.tile(axr,(np.size(tm),1)))\n",
    "\n",
    "    tm[(tm == np.min(tm)) & (bf == 0)] = np.inf\n",
    "\n",
    "    adc_prime = adc_tiled * (1 - sigma_tiled* np.exp(-tm*axr_tiled))\n",
    "    normalised_signal = np.exp(-adc_prime * be)\n",
    "    \n",
    "    return normalised_signal, adc_prime\n",
    "\n",
    "def sim_sig_pytorch_new(bf,be,tm,adc,sigma,axr,nvox):\n",
    "    \n",
    "    be_tiled = torch.tile(be,(nvox,1))\n",
    "    bf_tiled = torch.tile(bf,(nvox,1))\n",
    "    tm_tiled = torch.tile(tm,(nvox,1))\n",
    "\n",
    "    \n",
    "    \"\"\"adc_tiled = torch.tile(adc,(tm.shape[0],1)).t()\n",
    "    sigma_tiled = torch.tile(sigma,(tm.shape[0],1)).t()\n",
    "    axr_tiled = torch.tile(axr,(tm.shape[0],1)).t()\"\"\"\n",
    "\n",
    "    adc_tiled = adc\n",
    "    axr_tiled = axr\n",
    "    sigma_tiled = sigma\n",
    "\n",
    "    tm_tiled[(tm_tiled == torch.min(tm_tiled)) & (bf_tiled == 0)] = torch.inf\n",
    "\n",
    "    \"\"\"print(\"be: \", be_tiled.shape)\n",
    "    print(\"bf: \", bf_tiled.shape)\n",
    "    print(\"tm: \", tm_tiled.shape)\n",
    "    print(\"adc: \", adc_tiled.shape)\n",
    "    print(\"sigma: \", sigma_tiled.shape)\n",
    "    print(\"axr: \", axr_tiled.shape)\"\"\"\n",
    "\n",
    "\n",
    "    adc_prime_tiled = adc_tiled * (1 - sigma_tiled*torch.exp(-tm_tiled*axr_tiled))\n",
    "    normalised_signal_tiled = torch.exp(-adc_prime_tiled * be_tiled)\n",
    "    \n",
    "    return normalised_signal_tiled, adc_prime_tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sig_pytorch(bf, be, tm, adc, sigma, axr, batch_size):\n",
    "    \n",
    "    #this is while error checking, do the conversion outside the function later on.\n",
    "    \"\"\"adc = torch.tensor(adc)\n",
    "    sigma = torch.tensor(sigma)\n",
    "    axr = torch.tensor(axr)\n",
    "    be = torch.tensor(be)\n",
    "    bf = torch.tensor(bf)\n",
    "    tm = torch.tensor(tm)\"\"\"\n",
    "    \n",
    "    #be, bf & tm start as [8,] \n",
    "    be_tiled = torch.tile(be,(batch_size,1))\n",
    "    bf_tiled = torch.tile(bf,(batch_size,1))\n",
    "    tm_tiled = torch.tile(tm,(batch_size,1))\n",
    "    #print(\"be\",be_tiled.size())\n",
    "    #be, bf & tm tiled are batch size x 8\n",
    "\n",
    "\n",
    "    adc_tiled = torch.tile(adc,(1,tm.shape[0]))\n",
    "    sigma_tiled = torch.tile(sigma,(1,tm.shape[0]))\n",
    "    axr_tiled = torch.tile(axr,(1,tm.shape[0]))\n",
    "    #print(\"adc\",adc_tiled.size())\n",
    "    \n",
    "\n",
    "    tm_tiled[(tm_tiled == torch.min(tm_tiled)) & (bf_tiled == 0)] = float('inf')\n",
    "    \n",
    "\n",
    "    adc_prime_tiled = adc_tiled * (1 - sigma_tiled * torch.exp(-tm_tiled * axr_tiled))\n",
    "    normalised_signal_tiled = torch.exp(-adc_prime_tiled * be_tiled)\n",
    "\n",
    "    return normalised_signal_tiled, adc_prime_tiled\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 1000 # number of voxels to simulate\n",
    "\n",
    "bf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e-3   # filter b-values [ms/um2]\n",
    "be = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e-3       # encoding b-values [ms/um2]\n",
    "tm = np.array([20, 20, 20, 20, 200, 200, 400, 400], dtype=np.float32) * 1e-3 # mixing time [s]\n",
    "\n",
    "adc_lb = 0.1        #[um2/ms]\n",
    "adc_ub = 3.5        #[um2/ms]\n",
    "sig_lb = 0          #[a.u.]\n",
    "sig_ub = 1          #[a.u.]\n",
    "axr_lb = 0.1        #[s-1]\n",
    "axr_ub = 20         #[s-1]\n",
    "\n",
    "#consider doing in si units\n",
    "\n",
    "limits = np.array([[adc_lb, adc_ub], [sig_lb, sig_ub] , [axr_lb, axr_ub]])\n",
    "\n",
    "adc_init = (adc_lb + adc_ub) / 2 #[um2/ms]\n",
    "sig_init = (sig_lb + sig_ub) / 2 #[a.u.]\n",
    "axr_init = (axr_lb + axr_ub) / 2 #[ms-1]\n",
    "\n",
    "num_inits = 5\n",
    "\n",
    "# Create equally spaced arrays for each parameter\n",
    "# remove first and last values which are on the \"face of the cube\"\n",
    "adc_inits = np.linspace(adc_lb, adc_ub, num_inits)[1:-1]\n",
    "sig_inits = np.linspace(sig_lb, sig_ub, num_inits)[1:-1]\n",
    "axr_inits = np.linspace(axr_lb, axr_ub, num_inits)[1:-1]\n",
    "\n",
    "# Generate all permutations of combinations\n",
    "all_inits = list(product(adc_inits, sig_inits, axr_inits))\n",
    "\n",
    "# Convert the list of tuples to a NumPy array\n",
    "all_inits = np.array(all_inits)\n",
    "\n",
    "sim_adc = np.random.uniform(adc_lb,adc_ub,nvox)                 # ADC, simulated [um2/ms]\n",
    "sim_sigma = np.random.uniform(sig_lb,sig_ub,nvox)               # sigma, simulated [a.u.]\n",
    "sim_axr = np.random.uniform(axr_lb,axr_ub,nvox)                 # AXR, simulated [ms-1]\n",
    "\n",
    "sim_E_vox, sim_adc_prime = sim_sig_np(bf,be,tm,sim_adc,sim_sigma,sim_axr,nvox)\n",
    "\n",
    "\n",
    "#a_test1,a_test2 = sim_sig_pytorch(bf,be,tm,sim_adc,sim_sigma,sim_axr,len(sim_axr))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_bins=20\n",
    "#make 200 and voxels 100,000 to see adc prime dropoff\n",
    "fig, axs = plt.subplots(1, 3, sharey=True,figsize=(15, 5))\n",
    "axs[0].hist(sim_adc, bins=n_bins)\n",
    "axs[1].hist(sim_sigma, bins=n_bins)\n",
    "axs[2].hist(sim_axr, bins=n_bins)\n",
    "axs[0].set_title('ADC Histogram ')\n",
    "axs[0].set_xlabel('ADC Values [um^2/ms]')\n",
    "axs[1].set_title('Sigma Histogram')\n",
    "axs[1].set_xlabel('Sigma Values [arbitrary units]')\n",
    "axs[2].set_title('AXR Histogram')\n",
    "axs[2].set_xlabel('AXR Values [ms-1]');\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3,figsize=(15, 5))\n",
    "axs[0].hist(sim_E_vox.flatten(), bins=n_bins)\n",
    "axs[0].set_title('Signal Histogram ')\n",
    "axs[0].set_xlabel('Signal Values [units?]')\n",
    "axs[1].hist(sim_E_vox[sim_E_vox != 1].flatten(), bins=n_bins)\n",
    "axs[1].set_title('Signal Histogram with ones removed')\n",
    "axs[1].set_xlabel('Signal Values [units?]')\n",
    "axs[2].hist(sim_adc_prime.flatten(), bins=n_bins)\n",
    "axs[2].set_title('ADC prime Histogram')\n",
    "axs[2].set_xlabel('ADC prime Values [units?]');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting b-value against normalised signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([be[0], be[1]], [sim_E_vox[0,0], sim_E_vox[0,1]], 'bo-')\n",
    "plt.annotate(tm[0], (be[1], sim_E_vox[0, 1]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[2], be[3]], [sim_E_vox[0,2], sim_E_vox[0,3]], 'go-')\n",
    "plt.annotate(tm[2], (be[3], sim_E_vox[0, 3]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[4], be[5]], [sim_E_vox[0,4], sim_E_vox[0,5]], 'ko-')\n",
    "plt.annotate(tm[4], (be[5], sim_E_vox[0, 5]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[6], be[7]], [sim_E_vox[0,6], sim_E_vox[0,7]], 'mo-')\n",
    "plt.annotate(tm[6], (be[7], sim_E_vox[0, 7]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "\n",
    "#plt.title('Scatter plot with 4 lines')\n",
    "plt.xlabel('Encoding block b-value [m2/s]')\n",
    "#are units correct\n",
    "plt.ylabel('Normalised Signal (sum of the magnetisations)')\n",
    "#unit?\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse_adc_prime_1_vox(variables_to_optimize, tm, bf, be, smeas):\n",
    "    # For the signal from 1 voxel.\n",
    "    adc_est, sigma_est, axr_est = variables_to_optimize\n",
    "    _ , adc_tm_fit = sim_sig_np_1_vox(bf,be,tm,adc_est,sigma_est,axr_est)\n",
    "\n",
    "    bf_tm = np.column_stack((bf.flatten(), tm.flatten()))\n",
    "\n",
    "    # Find unique rows and corresponding indices\n",
    "    univols, univols_indices = np.unique(bf_tm, axis=0, return_index=True)\n",
    "\n",
    "    nsf = univols.shape[0]\n",
    "\n",
    "    ix1 = np.where((np.sum(univols[:, None, :] == bf_tm, axis=2) == 2) & (be == 0))[1]\n",
    "    ix2 = np.where((np.sum(univols[:, None, :] == bf_tm, axis=2) == 2) & (be > 0))[1]\n",
    "    \n",
    "    #this line is hardcoded\n",
    "    smeas = smeas.reshape(8)\n",
    "\n",
    "    adc_tm_calc = -1 / (be[ix2] - be[ix1]) * np.log(smeas[ix2] / smeas[ix1])\n",
    "\n",
    "    #this line is hardcoded\n",
    "    adc_tm_fit = adc_tm_fit[:, ::2]\n",
    "\n",
    "    sse = np.sum((adc_tm_calc - adc_tm_fit) ** 2)\n",
    "    return sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(xk):\n",
    "    print('Estimates of ADC, sigma, AXR:    ', xk)\n",
    "\n",
    "# Define the bounds for adc, sigma, axr\n",
    "bounds = tuple(map(tuple, limits.tolist())) #this line seems messy\n",
    "\n",
    "NLLS_adc_all = np.empty(shape=(0,))\n",
    "NLLS_sigma_all = np.empty(shape=(0,))\n",
    "NLLS_axr_all = np.empty(shape=(0,))\n",
    "    \n",
    "NLLS_adc_prime_all = np.empty(shape=(nvox,8))\n",
    "NLLS_E_vox_all = np.empty(shape= (nvox,8))\n",
    "\n",
    "sses = np.array([])\n",
    "for current_vox in range(nvox):\n",
    "    #reset best for each voxel\n",
    "    best_sse = 1\n",
    "\n",
    "    #extract relevant info for current voxel\n",
    "    cur_E_vox = sim_E_vox[current_vox,:]\n",
    "    cur_adc_prime = sim_adc_prime[current_vox,:]\n",
    "\n",
    "    cur_adc = sim_adc[current_vox]\n",
    "    cur_sigma = sim_sigma[current_vox]\n",
    "    cur_axr = sim_axr[current_vox]\n",
    "\n",
    "    additional_args_1_vox = (tm, bf, be, cur_E_vox) \n",
    "\n",
    "    for combination in range(all_inits.shape[0]):\n",
    "        inits = all_inits[combination,:]\n",
    "        \n",
    "        result_1_vox = scipy.optimize.minimize(sse_adc_prime_1_vox, inits, args=additional_args_1_vox, bounds=bounds)\n",
    "\n",
    "        if result_1_vox.fun < best_sse:\n",
    "            best_sse = result_1_vox.fun\n",
    "            NLLS_cur_adc, NLLS_cur_sigma, NLLS_cur_axr = result_1_vox.x\n",
    "    \n",
    "    sses = np.append(sses,best_sse)\n",
    "    # note the 1 instead of nvox, because it is for 1 voxel \n",
    "    NLLS_cur_E_vox, NLLS_cur_adc_prime = sim_sig_np_1_vox(bf,be,tm,NLLS_cur_adc, NLLS_cur_sigma, NLLS_cur_axr)\n",
    "\n",
    "    NLLS_adc_all = np.append(NLLS_adc_all, NLLS_cur_adc)\n",
    "    NLLS_sigma_all = np.append(NLLS_sigma_all, NLLS_cur_sigma)\n",
    "    NLLS_axr_all = np.append(NLLS_axr_all, NLLS_cur_axr)\n",
    "    \n",
    "    NLLS_adc_prime_all[current_vox,:] = NLLS_cur_adc_prime\n",
    "    NLLS_E_vox_all[current_vox,:] = NLLS_cur_E_vox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLS Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_37064/1703121747.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Debugging sse and seeing how large it is for the worst values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msses_descending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sses' is not defined"
     ]
    }
   ],
   "source": [
    "#Debugging sse and seeing how large it is for the worst values. \n",
    "sses.sort()\n",
    "sses_descending = sses[::-1]\n",
    "\n",
    "\n",
    "\"\"\"MAYBE ADD LOSS PER 'EPOCH' FOR NLLS\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(loss_progress) + 1), loss_progress, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\"\n",
    "\n",
    "\n",
    "#plotting the sse\n",
    "plt.figure()\n",
    "plt.hist(sses.flatten(), bins=200)\n",
    "\n",
    "plt.figure()\n",
    "# for first voxel\n",
    "plt.scatter(be, sim_E_vox[0,:], label='simulated')\n",
    "plt.scatter(be, NLLS_E_vox_all[0,:], label='predicted')\n",
    "plt.xlabel(\"be\")\n",
    "plt.ylabel(\"tm\")\n",
    "plt.legend()\n",
    "\n",
    "# plot scatter plots to analyse correlation of predicted free params against ground truth\n",
    "plt.figure()\n",
    "\n",
    "param_sim = [sim_adc, sim_sigma, sim_axr]\n",
    "param_pred = [NLLS_adc_all, NLLS_sigma_all, NLLS_axr_all]\n",
    "param_name = ['ADC', 'Sigma', 'AXR']\n",
    "\n",
    "rvals = []\n",
    "\n",
    "for i,_ in enumerate(param_sim):\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "    plt.scatter(param_sim[i], param_pred[i], s=2, c='navy')\n",
    "    plt.xlabel(param_name[i] + ' Ground Truth')\n",
    "    plt.ylabel(param_name[i] + ' Prediction')\n",
    "    #check what line below does. Commented out because it gave an error when using 1 voxel\n",
    "    #rvals.append(scipy.stats.pearsonr(np.squeeze(param_sim[i]), np.squeeze(param_pred[i])))\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "print(rvals)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # this is the neural network\n",
    "    #defining the init and foward pass functions. \n",
    "\n",
    "    def __init__(self,be,bf,tm,nparams,limits):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.be = be\n",
    "        self.bf = bf\n",
    "        self.tm = tm\n",
    "        self.limits = limits\n",
    "\n",
    "        #defining the layers that we want. \n",
    "        # 3 layers with no. of be nodes. \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.layers.extend([nn.Linear(len(be), len(be)), nn.PReLU()])\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html\n",
    "        self.encoder = nn.Sequential(*self.layers, nn.Linear(len(be), nparams))\n",
    "\n",
    "    def forward(self, E_vox):\n",
    "\n",
    "        params = torch.nn.functional.softplus(self.encoder(E_vox))\n",
    "        #running a forward pass through the network\n",
    "\n",
    "        #SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive\n",
    "        #params contains batch_size x nparams outputs, so each row is adc, sigma and axr.\n",
    "\n",
    "        #unsqueeze adds an additional dimension. \n",
    "        #parameter constraints from Elizabeth matlab \n",
    "\n",
    "        adc = torch.clamp(params[:, 0].unsqueeze(1), min=limits[0,0], max=limits[0,1])\n",
    "        sigma = torch.clamp(params[:, 1].unsqueeze(1), min=limits[1,0], max=limits[1,1])\n",
    "        axr = torch.clamp(params[:, 2].unsqueeze(1), min=limits[2,0], max=limits[2,1])\n",
    "        \n",
    "        \"\"\"print(adc[0,:])\n",
    "        print(sigma[0,:])\n",
    "        print(axr[0,:])\"\"\"\n",
    "\n",
    "        E_vox,_ = sim_sig_pytorch(self.bf, self.be, self.tm, adc, sigma, axr, axr.shape[0])\n",
    "        #E_vox, adc_prime = sim_sig_pytorch(self.bf, self.be, self.tm, adc, sigma, axr, axr.shape[0])\n",
    "\n",
    "        #axr.shape[0] is either the batch size or nvox\n",
    "\n",
    "        return E_vox, adc, sigma, axr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "nparams = 3\n",
    "#because of adc, sigma and axr\n",
    "\n",
    "#converting numpy arrays to pytorch tensors. \n",
    "be = torch.tensor(be)\n",
    "bf = torch.tensor(bf)\n",
    "tm = torch.tensor(tm)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "#initilise network\n",
    "net = Net(be, bf, tm, nparams,limits)\n",
    "\n",
    "#create batch queues for data\n",
    "#// means divide and round down. \n",
    "num_batches = len(sim_E_vox) // batch_size\n",
    "\n",
    "#import the sim_E_vox array into the dataloader\n",
    "#drop_last ignores the last batch if it is the wrong size. \n",
    "#num_workers is about performance. \n",
    "\n",
    "trainloader = utils.DataLoader(torch.from_numpy(sim_E_vox.astype(np.float32)),\n",
    "                                batch_size = batch_size, \n",
    "                                shuffle = True,\n",
    "                                num_workers = 0, #was 2 previously\n",
    "                                drop_last = True)\n",
    "\n",
    "# loss function and optimizer\n",
    "#choosing which loss function to use. \n",
    "#not sure what the optmizer is\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "# best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "#can increase patience a lot, speed not an issue.\n",
    "patience = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "epoch: 0; bad epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 106.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5735, 1.0000,  ..., 0.6064, 1.0000, 0.5788],\n",
      "        [1.0000, 0.8425, 1.0000,  ..., 0.8480, 1.0000, 0.8428],\n",
      "        [1.0000, 0.5524, 1.0000,  ..., 0.5761, 1.0000, 0.5542],\n",
      "        ...,\n",
      "        [1.0000, 0.8003, 1.0000,  ..., 0.8166, 1.0000, 0.8019],\n",
      "        [1.0000, 0.5352, 1.0000,  ..., 0.5542, 1.0000, 0.5416],\n",
      "        [1.0000, 0.7204, 1.0000,  ..., 0.7558, 1.0000, 0.7284]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[1.0000, 0.8642, 1.0000,  ..., 0.9645, 1.0000, 0.9530],\n",
      "        [1.0000, 0.8641, 1.0000,  ..., 0.9642, 1.0000, 0.9528],\n",
      "        [1.0000, 0.8642, 1.0000,  ..., 0.9644, 1.0000, 0.9530],\n",
      "        ...,\n",
      "        [1.0000, 0.8642, 1.0000,  ..., 0.9642, 1.0000, 0.9528],\n",
      "        [1.0000, 0.8642, 1.0000,  ..., 0.9646, 1.0000, 0.9532],\n",
      "        [1.0000, 0.8642, 1.0000,  ..., 0.9644, 1.0000, 0.9529]],\n",
      "       dtype=torch.float64, grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4946, 1.0000,  ..., 0.5761, 1.0000, 0.5112],\n",
      "        [1.0000, 0.9239, 1.0000,  ..., 0.9276, 1.0000, 0.9241],\n",
      "        [1.0000, 0.8387, 1.0000,  ..., 0.8468, 1.0000, 0.8464],\n",
      "        ...,\n",
      "        [1.0000, 0.4198, 1.0000,  ..., 0.4221, 1.0000, 0.4206],\n",
      "        [1.0000, 0.5918, 1.0000,  ..., 0.5945, 1.0000, 0.5919],\n",
      "        [1.0000, 0.5598, 1.0000,  ..., 0.5620, 1.0000, 0.5602]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5695, 1.0000,  ..., 0.5766, 1.0000, 0.5698],\n",
      "        [1.0000, 0.7042, 1.0000,  ..., 0.7523, 1.0000, 0.7150],\n",
      "        [1.0000, 0.5031, 1.0000,  ..., 0.5277, 1.0000, 0.5063],\n",
      "        ...,\n",
      "        [1.0000, 0.5378, 1.0000,  ..., 0.5401, 1.0000, 0.5381],\n",
      "        [1.0000, 0.5009, 1.0000,  ..., 0.5089, 1.0000, 0.5011],\n",
      "        [1.0000, 0.4587, 1.0000,  ..., 0.4768, 1.0000, 0.4620]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4331, 1.0000,  ..., 0.4428, 1.0000, 0.4341],\n",
      "        [1.0000, 0.4300, 1.0000,  ..., 0.4325, 1.0000, 0.4301],\n",
      "        [1.0000, 0.9688, 1.0000,  ..., 0.9690, 1.0000, 0.9689],\n",
      "        ...,\n",
      "        [1.0000, 0.7653, 1.0000,  ..., 0.7754, 1.0000, 0.7658],\n",
      "        [1.0000, 0.7561, 1.0000,  ..., 0.7648, 1.0000, 0.7568],\n",
      "        [1.0000, 0.5363, 1.0000,  ..., 0.5507, 1.0000, 0.5371]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8977, 1.0000,  ..., 0.8986, 1.0000, 0.8977],\n",
      "        [1.0000, 0.7051, 1.0000,  ..., 0.7077, 1.0000, 0.7051],\n",
      "        [1.0000, 0.5214, 1.0000,  ..., 0.5248, 1.0000, 0.5215],\n",
      "        ...,\n",
      "        [1.0000, 0.6840, 1.0000,  ..., 0.7699, 1.0000, 0.7450],\n",
      "        [1.0000, 0.9659, 1.0000,  ..., 0.9743, 1.0000, 0.9725],\n",
      "        [1.0000, 0.4338, 1.0000,  ..., 0.4386, 1.0000, 0.4340]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5009, 1.0000,  ..., 0.6161, 1.0000, 0.5553],\n",
      "        [1.0000, 0.7978, 1.0000,  ..., 0.8021, 1.0000, 0.7979],\n",
      "        [1.0000, 0.8713, 1.0000,  ..., 0.8796, 1.0000, 0.8727],\n",
      "        ...,\n",
      "        [1.0000, 0.7382, 1.0000,  ..., 0.7395, 1.0000, 0.7382],\n",
      "        [1.0000, 0.8711, 1.0000,  ..., 0.8728, 1.0000, 0.8712],\n",
      "        [1.0000, 0.6279, 1.0000,  ..., 0.8033, 1.0000, 0.7695]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4998, 1.0000,  ..., 0.5154, 1.0000, 0.5024],\n",
      "        [1.0000, 0.6067, 1.0000,  ..., 0.6238, 1.0000, 0.6079],\n",
      "        [1.0000, 0.8346, 1.0000,  ..., 0.8398, 1.0000, 0.8349],\n",
      "        ...,\n",
      "        [1.0000, 0.8639, 1.0000,  ..., 0.8641, 1.0000, 0.8639],\n",
      "        [1.0000, 0.8874, 1.0000,  ..., 0.9295, 1.0000, 0.9068],\n",
      "        [1.0000, 0.6141, 1.0000,  ..., 0.6158, 1.0000, 0.6141]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 1; bad epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 111.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5461, 1.0000,  ..., 0.5620, 1.0000, 0.5480],\n",
      "        [1.0000, 0.4899, 1.0000,  ..., 0.5616, 1.0000, 0.5102],\n",
      "        [1.0000, 0.5682, 1.0000,  ..., 0.5723, 1.0000, 0.5683],\n",
      "        ...,\n",
      "        [1.0000, 0.5559, 1.0000,  ..., 0.6150, 1.0000, 0.5862],\n",
      "        [1.0000, 0.4706, 1.0000,  ..., 0.4877, 1.0000, 0.4720],\n",
      "        [1.0000, 0.7061, 1.0000,  ..., 0.7451, 1.0000, 0.7226]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.9007, 1.0000,  ..., 0.9044, 1.0000, 0.9010],\n",
      "        [1.0000, 0.7903, 1.0000,  ..., 0.8097, 1.0000, 0.7977],\n",
      "        [1.0000, 0.9453, 1.0000,  ..., 0.9456, 1.0000, 0.9453],\n",
      "        ...,\n",
      "        [1.0000, 0.6059, 1.0000,  ..., 0.6121, 1.0000, 0.6061],\n",
      "        [1.0000, 0.5215, 1.0000,  ..., 0.5221, 1.0000, 0.5215],\n",
      "        [1.0000, 0.6355, 1.0000,  ..., 0.6644, 1.0000, 0.6386]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6110, 1.0000,  ..., 0.8979, 1.0000, 0.8765],\n",
      "        [1.0000, 0.7417, 1.0000,  ..., 0.7449, 1.0000, 0.7417],\n",
      "        [1.0000, 0.6012, 1.0000,  ..., 0.6102, 1.0000, 0.6015],\n",
      "        ...,\n",
      "        [1.0000, 0.6656, 1.0000,  ..., 0.6715, 1.0000, 0.6674],\n",
      "        [1.0000, 0.7163, 1.0000,  ..., 0.7452, 1.0000, 0.7205],\n",
      "        [1.0000, 0.5724, 1.0000,  ..., 0.5751, 1.0000, 0.5725]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6622, 1.0000,  ..., 0.7138, 1.0000, 0.6752],\n",
      "        [1.0000, 0.6076, 1.0000,  ..., 0.6139, 1.0000, 0.6077],\n",
      "        [1.0000, 0.6865, 1.0000,  ..., 0.7693, 1.0000, 0.7255],\n",
      "        ...,\n",
      "        [1.0000, 0.9658, 1.0000,  ..., 0.9679, 1.0000, 0.9660],\n",
      "        [1.0000, 0.5801, 1.0000,  ..., 0.5999, 1.0000, 0.5831],\n",
      "        [1.0000, 0.5473, 1.0000,  ..., 0.5486, 1.0000, 0.5479]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5596, 1.0000,  ..., 0.5618, 1.0000, 0.5596],\n",
      "        [1.0000, 0.4451, 1.0000,  ..., 0.5155, 1.0000, 0.4982],\n",
      "        [1.0000, 0.7139, 1.0000,  ..., 0.7146, 1.0000, 0.7139],\n",
      "        ...,\n",
      "        [1.0000, 0.4942, 1.0000,  ..., 0.5031, 1.0000, 0.4968],\n",
      "        [1.0000, 0.6729, 1.0000,  ..., 0.7356, 1.0000, 0.6964],\n",
      "        [1.0000, 0.9575, 1.0000,  ..., 0.9691, 1.0000, 0.9665]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7469, 1.0000,  ..., 0.7558, 1.0000, 0.7484],\n",
      "        [1.0000, 0.8738, 1.0000,  ..., 0.8802, 1.0000, 0.8743],\n",
      "        [1.0000, 0.5115, 1.0000,  ..., 0.5128, 1.0000, 0.5115],\n",
      "        ...,\n",
      "        [1.0000, 0.5499, 1.0000,  ..., 0.5574, 1.0000, 0.5501],\n",
      "        [1.0000, 0.8467, 1.0000,  ..., 0.8478, 1.0000, 0.8467],\n",
      "        [1.0000, 0.4178, 1.0000,  ..., 0.4332, 1.0000, 0.4202]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5779, 1.0000,  ..., 0.6064, 1.0000, 0.5835],\n",
      "        [1.0000, 0.6646, 1.0000,  ..., 0.7255, 1.0000, 0.7201],\n",
      "        [1.0000, 0.6877, 1.0000,  ..., 0.7916, 1.0000, 0.7465],\n",
      "        ...,\n",
      "        [1.0000, 0.5843, 1.0000,  ..., 0.7080, 1.0000, 0.6675],\n",
      "        [1.0000, 0.7597, 1.0000,  ..., 0.7666, 1.0000, 0.7607],\n",
      "        [1.0000, 0.4995, 1.0000,  ..., 0.5546, 1.0000, 0.5079]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 2; bad epochs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4202, 1.0000,  ..., 0.4834, 1.0000, 0.4329],\n",
      "        [1.0000, 0.8979, 1.0000,  ..., 0.9435, 1.0000, 0.9386],\n",
      "        [1.0000, 0.4561, 1.0000,  ..., 0.4656, 1.0000, 0.4573],\n",
      "        ...,\n",
      "        [1.0000, 0.5372, 1.0000,  ..., 0.5462, 1.0000, 0.5374],\n",
      "        [1.0000, 0.8756, 1.0000,  ..., 0.9062, 1.0000, 0.8837],\n",
      "        [1.0000, 0.6317, 1.0000,  ..., 0.6429, 1.0000, 0.6322]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6710, 1.0000,  ..., 0.6765, 1.0000, 0.6711],\n",
      "        [1.0000, 0.4530, 1.0000,  ..., 0.4533, 1.0000, 0.4530],\n",
      "        [1.0000, 0.4364, 1.0000,  ..., 0.4386, 1.0000, 0.4365],\n",
      "        ...,\n",
      "        [1.0000, 0.4640, 1.0000,  ..., 0.4704, 1.0000, 0.4650],\n",
      "        [1.0000, 0.7214, 1.0000,  ..., 0.8230, 1.0000, 0.8097],\n",
      "        [1.0000, 0.4351, 1.0000,  ..., 0.4376, 1.0000, 0.4355]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7902, 1.0000,  ..., 0.8259, 1.0000, 0.8116],\n",
      "        [1.0000, 0.8320, 1.0000,  ..., 0.8504, 1.0000, 0.8347],\n",
      "        [1.0000, 0.4543, 1.0000,  ..., 0.4606, 1.0000, 0.4546],\n",
      "        ...,\n",
      "        [1.0000, 0.4899, 1.0000,  ..., 0.5616, 1.0000, 0.5102],\n",
      "        [1.0000, 0.4215, 1.0000,  ..., 0.4558, 1.0000, 0.4252],\n",
      "        [1.0000, 0.4565, 1.0000,  ..., 0.4686, 1.0000, 0.4571]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6854, 1.0000,  ..., 0.7907, 1.0000, 0.7479],\n",
      "        [1.0000, 0.5095, 1.0000,  ..., 0.5187, 1.0000, 0.5113],\n",
      "        [1.0000, 0.5894, 1.0000,  ..., 0.6231, 1.0000, 0.6009],\n",
      "        ...,\n",
      "        [1.0000, 0.4263, 1.0000,  ..., 0.4274, 1.0000, 0.4263],\n",
      "        [1.0000, 0.7978, 1.0000,  ..., 0.8021, 1.0000, 0.7979],\n",
      "        [1.0000, 0.6755, 1.0000,  ..., 0.6806, 1.0000, 0.6756]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5201, 1.0000,  ..., 0.5731, 1.0000, 0.5318],\n",
      "        [1.0000, 0.5760, 1.0000,  ..., 0.5795, 1.0000, 0.5763],\n",
      "        [1.0000, 0.5892, 1.0000,  ..., 0.9049, 1.0000, 0.8773],\n",
      "        ...,\n",
      "        [1.0000, 0.9162, 1.0000,  ..., 0.9198, 1.0000, 0.9165],\n",
      "        [1.0000, 0.6932, 1.0000,  ..., 0.7525, 1.0000, 0.7250],\n",
      "        [1.0000, 0.7003, 1.0000,  ..., 0.7051, 1.0000, 0.7007]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8221, 1.0000,  ..., 0.8276, 1.0000, 0.8224],\n",
      "        [1.0000, 0.9008, 1.0000,  ..., 0.9076, 1.0000, 0.9014],\n",
      "        [1.0000, 0.6882, 1.0000,  ..., 0.7426, 1.0000, 0.7140],\n",
      "        ...,\n",
      "        [1.0000, 0.8249, 1.0000,  ..., 0.8330, 1.0000, 0.8257],\n",
      "        [1.0000, 0.5950, 1.0000,  ..., 0.5970, 1.0000, 0.5950],\n",
      "        [1.0000, 0.6452, 1.0000,  ..., 0.6491, 1.0000, 0.6453]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6523, 1.0000,  ..., 0.7261, 1.0000, 0.6842],\n",
      "        [1.0000, 0.5202, 1.0000,  ..., 0.5625, 1.0000, 0.5525],\n",
      "        [1.0000, 0.6355, 1.0000,  ..., 0.6644, 1.0000, 0.6386],\n",
      "        ...,\n",
      "        [1.0000, 0.6428, 1.0000,  ..., 0.6884, 1.0000, 0.6569],\n",
      "        [1.0000, 0.8992, 1.0000,  ..., 0.9236, 1.0000, 0.9213],\n",
      "        [1.0000, 0.6763, 1.0000,  ..., 0.6777, 1.0000, 0.6764]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 166.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 3; bad epochs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7819, 1.0000,  ..., 0.7960, 1.0000, 0.7904],\n",
      "        [1.0000, 0.8010, 1.0000,  ..., 0.8013, 1.0000, 0.8010],\n",
      "        [1.0000, 0.8742, 1.0000,  ..., 0.8839, 1.0000, 0.8821],\n",
      "        ...,\n",
      "        [1.0000, 0.9709, 1.0000,  ..., 0.9869, 1.0000, 0.9806],\n",
      "        [1.0000, 0.6646, 1.0000,  ..., 0.7255, 1.0000, 0.7201],\n",
      "        [1.0000, 0.9253, 1.0000,  ..., 0.9264, 1.0000, 0.9254]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4414, 1.0000,  ..., 0.4861, 1.0000, 0.4543],\n",
      "        [1.0000, 0.8649, 1.0000,  ..., 0.8687, 1.0000, 0.8651],\n",
      "        [1.0000, 0.8457, 1.0000,  ..., 0.8571, 1.0000, 0.8497],\n",
      "        ...,\n",
      "        [1.0000, 0.5073, 1.0000,  ..., 0.5104, 1.0000, 0.5075],\n",
      "        [1.0000, 0.9007, 1.0000,  ..., 0.9044, 1.0000, 0.9010],\n",
      "        [1.0000, 0.4834, 1.0000,  ..., 0.4892, 1.0000, 0.4835]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5729, 1.0000,  ..., 0.5845, 1.0000, 0.5736],\n",
      "        [1.0000, 0.4241, 1.0000,  ..., 0.4243, 1.0000, 0.4241],\n",
      "        [1.0000, 0.5328, 1.0000,  ..., 0.6263, 1.0000, 0.5909],\n",
      "        ...,\n",
      "        [1.0000, 0.5824, 1.0000,  ..., 0.5882, 1.0000, 0.5827],\n",
      "        [1.0000, 0.7214, 1.0000,  ..., 0.8230, 1.0000, 0.8097],\n",
      "        [1.0000, 0.6319, 1.0000,  ..., 0.6400, 1.0000, 0.6326]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|  | 5/7 [00:00<00:00, 42.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5729, 1.0000,  ..., 0.5818, 1.0000, 0.5732],\n",
      "        [1.0000, 0.6074, 1.0000,  ..., 0.6108, 1.0000, 0.6076],\n",
      "        [1.0000, 0.5807, 1.0000,  ..., 0.6244, 1.0000, 0.5970],\n",
      "        ...,\n",
      "        [1.0000, 0.5745, 1.0000,  ..., 0.8486, 1.0000, 0.7871],\n",
      "        [1.0000, 0.9034, 1.0000,  ..., 0.9074, 1.0000, 0.9038],\n",
      "        [1.0000, 0.7824, 1.0000,  ..., 0.8039, 1.0000, 0.7853]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5779, 1.0000,  ..., 0.6064, 1.0000, 0.5835],\n",
      "        [1.0000, 0.6710, 1.0000,  ..., 0.6765, 1.0000, 0.6711],\n",
      "        [1.0000, 0.4687, 1.0000,  ..., 0.4692, 1.0000, 0.4687],\n",
      "        ...,\n",
      "        [1.0000, 0.6428, 1.0000,  ..., 0.6884, 1.0000, 0.6569],\n",
      "        [1.0000, 0.7126, 1.0000,  ..., 0.9177, 1.0000, 0.8951],\n",
      "        [1.0000, 0.7474, 1.0000,  ..., 0.7739, 1.0000, 0.7598]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4478, 1.0000,  ..., 0.4665, 1.0000, 0.4527],\n",
      "        [1.0000, 0.8741, 1.0000,  ..., 0.8795, 1.0000, 0.8757],\n",
      "        [1.0000, 0.8945, 1.0000,  ..., 0.9040, 1.0000, 0.8955],\n",
      "        ...,\n",
      "        [1.0000, 0.9732, 1.0000,  ..., 0.9799, 1.0000, 0.9752],\n",
      "        [1.0000, 0.9688, 1.0000,  ..., 0.9690, 1.0000, 0.9689],\n",
      "        [1.0000, 0.4709, 1.0000,  ..., 0.4726, 1.0000, 0.4710]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 35.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4209, 1.0000,  ..., 0.6181, 1.0000, 0.5458],\n",
      "        [1.0000, 0.7941, 1.0000,  ..., 0.8781, 1.0000, 0.8655],\n",
      "        [1.0000, 0.4667, 1.0000,  ..., 0.5033, 1.0000, 0.4752],\n",
      "        ...,\n",
      "        [1.0000, 0.8173, 1.0000,  ..., 0.8177, 1.0000, 0.8173],\n",
      "        [1.0000, 0.8491, 1.0000,  ..., 0.8666, 1.0000, 0.8533],\n",
      "        [1.0000, 0.6078, 1.0000,  ..., 0.6095, 1.0000, 0.6079]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 4; bad epochs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9500, 1.0000,  ..., 0.9570, 1.0000, 0.9510],\n",
      "        [1.0000, 0.4595, 1.0000,  ..., 0.4690, 1.0000, 0.4598],\n",
      "        [1.0000, 0.4442, 1.0000,  ..., 0.4502, 1.0000, 0.4443],\n",
      "        ...,\n",
      "        [1.0000, 0.9383, 1.0000,  ..., 0.9387, 1.0000, 0.9383],\n",
      "        [1.0000, 0.8329, 1.0000,  ..., 0.8488, 1.0000, 0.8348],\n",
      "        [1.0000, 0.9374, 1.0000,  ..., 0.9378, 1.0000, 0.9374]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7061, 1.0000,  ..., 0.7105, 1.0000, 0.7070],\n",
      "        [1.0000, 0.4198, 1.0000,  ..., 0.4221, 1.0000, 0.4206],\n",
      "        [1.0000, 0.4942, 1.0000,  ..., 0.5031, 1.0000, 0.4968],\n",
      "        ...,\n",
      "        [1.0000, 0.6460, 1.0000,  ..., 0.6554, 1.0000, 0.6469],\n",
      "        [1.0000, 0.4902, 1.0000,  ..., 0.5221, 1.0000, 0.4941],\n",
      "        [1.0000, 0.6992, 1.0000,  ..., 0.7014, 1.0000, 0.6993]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6451, 1.0000,  ..., 0.6469, 1.0000, 0.6454],\n",
      "        [1.0000, 0.5881, 1.0000,  ..., 0.6128, 1.0000, 0.6007],\n",
      "        [1.0000, 0.4664, 1.0000,  ..., 0.4880, 1.0000, 0.4676],\n",
      "        ...,\n",
      "        [1.0000, 0.8031, 1.0000,  ..., 0.8258, 1.0000, 0.8061],\n",
      "        [1.0000, 0.4400, 1.0000,  ..., 0.4443, 1.0000, 0.4402],\n",
      "        [1.0000, 0.8662, 1.0000,  ..., 0.8681, 1.0000, 0.8662]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6065, 1.0000,  ..., 0.6184, 1.0000, 0.6071],\n",
      "        [1.0000, 0.6601, 1.0000,  ..., 0.6666, 1.0000, 0.6604],\n",
      "        [1.0000, 0.7386, 1.0000,  ..., 0.7523, 1.0000, 0.7517],\n",
      "        ...,\n",
      "        [1.0000, 0.4931, 1.0000,  ..., 0.4936, 1.0000, 0.4932],\n",
      "        [1.0000, 0.4570, 1.0000,  ..., 0.4627, 1.0000, 0.4571],\n",
      "        [1.0000, 0.4953, 1.0000,  ..., 0.4989, 1.0000, 0.4954]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 69.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8267, 1.0000,  ..., 0.8281, 1.0000, 0.8269],\n",
      "        [1.0000, 0.6411, 1.0000,  ..., 0.6509, 1.0000, 0.6425],\n",
      "        [1.0000, 0.8945, 1.0000,  ..., 0.9040, 1.0000, 0.8955],\n",
      "        ...,\n",
      "        [1.0000, 0.6471, 1.0000,  ..., 0.7449, 1.0000, 0.6816],\n",
      "        [1.0000, 0.7533, 1.0000,  ..., 0.7648, 1.0000, 0.7547],\n",
      "        [1.0000, 0.8655, 1.0000,  ..., 0.8665, 1.0000, 0.8656]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4489, 1.0000,  ..., 0.4549, 1.0000, 0.4496],\n",
      "        [1.0000, 0.5644, 1.0000,  ..., 0.5825, 1.0000, 0.5659],\n",
      "        [1.0000, 0.8425, 1.0000,  ..., 0.8480, 1.0000, 0.8428],\n",
      "        ...,\n",
      "        [1.0000, 0.7521, 1.0000,  ..., 0.7775, 1.0000, 0.7767],\n",
      "        [1.0000, 0.8992, 1.0000,  ..., 0.9236, 1.0000, 0.9213],\n",
      "        [1.0000, 0.4500, 1.0000,  ..., 0.4553, 1.0000, 0.4501]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6103, 1.0000,  ..., 0.6130, 1.0000, 0.6105],\n",
      "        [1.0000, 0.4406, 1.0000,  ..., 0.4557, 1.0000, 0.4420],\n",
      "        [1.0000, 0.6199, 1.0000,  ..., 0.6242, 1.0000, 0.6200],\n",
      "        ...,\n",
      "        [1.0000, 0.8495, 1.0000,  ..., 0.9149, 1.0000, 0.8827],\n",
      "        [1.0000, 0.4842, 1.0000,  ..., 0.5187, 1.0000, 0.4882],\n",
      "        [1.0000, 0.8792, 1.0000,  ..., 0.9011, 1.0000, 0.8837]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 5; bad epochs: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9652, 1.0000,  ..., 0.9736, 1.0000, 0.9702],\n",
      "        [1.0000, 0.8112, 1.0000,  ..., 0.8602, 1.0000, 0.8395],\n",
      "        [1.0000, 0.5092, 1.0000,  ..., 0.5648, 1.0000, 0.5182],\n",
      "        ...,\n",
      "        [1.0000, 0.8267, 1.0000,  ..., 0.8281, 1.0000, 0.8269],\n",
      "        [1.0000, 0.8829, 1.0000,  ..., 0.8836, 1.0000, 0.8829],\n",
      "        [1.0000, 0.7283, 1.0000,  ..., 0.7691, 1.0000, 0.7497]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6622, 1.0000,  ..., 0.7138, 1.0000, 0.6752],\n",
      "        [1.0000, 0.4783, 1.0000,  ..., 0.5040, 1.0000, 0.4855],\n",
      "        [1.0000, 0.9393, 1.0000,  ..., 0.9553, 1.0000, 0.9507],\n",
      "        ...,\n",
      "        [1.0000, 0.4350, 1.0000,  ..., 0.4740, 1.0000, 0.4402],\n",
      "        [1.0000, 0.7593, 1.0000,  ..., 0.7690, 1.0000, 0.7604],\n",
      "        [1.0000, 0.4273, 1.0000,  ..., 0.4967, 1.0000, 0.4403]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4877, 1.0000,  ..., 0.6003, 1.0000, 0.5329],\n",
      "        [1.0000, 0.4352, 1.0000,  ..., 0.5187, 1.0000, 0.4735],\n",
      "        [1.0000, 0.5396, 1.0000,  ..., 0.5942, 1.0000, 0.5563],\n",
      "        ...,\n",
      "        [1.0000, 0.4656, 1.0000,  ..., 0.5016, 1.0000, 0.5003],\n",
      "        [1.0000, 0.7971, 1.0000,  ..., 0.7988, 1.0000, 0.7971],\n",
      "        [1.0000, 0.9520, 1.0000,  ..., 0.9704, 1.0000, 0.9593]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5328, 1.0000,  ..., 0.6263, 1.0000, 0.5909],\n",
      "        [1.0000, 0.7581, 1.0000,  ..., 0.7670, 1.0000, 0.7585],\n",
      "        [1.0000, 0.4640, 1.0000,  ..., 0.4704, 1.0000, 0.4650],\n",
      "        ...,\n",
      "        [1.0000, 0.5354, 1.0000,  ..., 0.7273, 1.0000, 0.7112],\n",
      "        [1.0000, 0.8324, 1.0000,  ..., 0.8331, 1.0000, 0.8324],\n",
      "        [1.0000, 0.5763, 1.0000,  ..., 0.5862, 1.0000, 0.5767]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8380, 1.0000,  ..., 0.8382, 1.0000, 0.8380],\n",
      "        [1.0000, 0.9335, 1.0000,  ..., 0.9343, 1.0000, 0.9336],\n",
      "        [1.0000, 0.5965, 1.0000,  ..., 0.6070, 1.0000, 0.5977],\n",
      "        ...,\n",
      "        [1.0000, 0.6078, 1.0000,  ..., 0.6480, 1.0000, 0.6140],\n",
      "        [1.0000, 0.9190, 1.0000,  ..., 0.9343, 1.0000, 0.9229],\n",
      "        [1.0000, 0.7431, 1.0000,  ..., 0.7569, 1.0000, 0.7462]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6724, 1.0000,  ..., 0.6752, 1.0000, 0.6725],\n",
      "        [1.0000, 0.5579, 1.0000,  ..., 0.6764, 1.0000, 0.6157],\n",
      "        [1.0000, 0.7666, 1.0000,  ..., 0.7705, 1.0000, 0.7668],\n",
      "        ...,\n",
      "        [1.0000, 0.4410, 1.0000,  ..., 0.4632, 1.0000, 0.4574],\n",
      "        [1.0000, 0.8352, 1.0000,  ..., 0.8355, 1.0000, 0.8352],\n",
      "        [1.0000, 0.7446, 1.0000,  ..., 0.7515, 1.0000, 0.7449]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5684, 1.0000,  ..., 0.6637, 1.0000, 0.5971],\n",
      "        [1.0000, 0.6593, 1.0000,  ..., 0.6662, 1.0000, 0.6600],\n",
      "        [1.0000, 0.5596, 1.0000,  ..., 0.5618, 1.0000, 0.5596],\n",
      "        ...,\n",
      "        [1.0000, 0.7340, 1.0000,  ..., 0.7417, 1.0000, 0.7347],\n",
      "        [1.0000, 0.6633, 1.0000,  ..., 0.6658, 1.0000, 0.6633],\n",
      "        [1.0000, 0.9039, 1.0000,  ..., 0.9077, 1.0000, 0.9055]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 90.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 6; bad epochs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5989, 1.0000,  ..., 0.6375, 1.0000, 0.6051],\n",
      "        [1.0000, 0.4209, 1.0000,  ..., 0.6181, 1.0000, 0.5458],\n",
      "        [1.0000, 0.4667, 1.0000,  ..., 0.5033, 1.0000, 0.4752],\n",
      "        ...,\n",
      "        [1.0000, 0.8738, 1.0000,  ..., 0.8802, 1.0000, 0.8743],\n",
      "        [1.0000, 0.6698, 1.0000,  ..., 0.6851, 1.0000, 0.6797],\n",
      "        [1.0000, 0.5474, 1.0000,  ..., 0.5550, 1.0000, 0.5514]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4445, 1.0000,  ..., 0.5770, 1.0000, 0.4869],\n",
      "        [1.0000, 0.9190, 1.0000,  ..., 0.9343, 1.0000, 0.9229],\n",
      "        [1.0000, 0.4829, 1.0000,  ..., 0.4911, 1.0000, 0.4837],\n",
      "        ...,\n",
      "        [1.0000, 0.6428, 1.0000,  ..., 0.6884, 1.0000, 0.6569],\n",
      "        [1.0000, 0.8010, 1.0000,  ..., 0.8013, 1.0000, 0.8010],\n",
      "        [1.0000, 0.9064, 1.0000,  ..., 0.9094, 1.0000, 0.9066]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 109.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4686, 1.0000,  ..., 0.4719, 1.0000, 0.4687],\n",
      "        [1.0000, 0.8167, 1.0000,  ..., 0.8245, 1.0000, 0.8173],\n",
      "        [1.0000, 0.7276, 1.0000,  ..., 0.7357, 1.0000, 0.7282],\n",
      "        ...,\n",
      "        [1.0000, 0.7597, 1.0000,  ..., 0.7666, 1.0000, 0.7607],\n",
      "        [1.0000, 0.6067, 1.0000,  ..., 0.6238, 1.0000, 0.6079],\n",
      "        [1.0000, 0.6944, 1.0000,  ..., 0.6987, 1.0000, 0.6948]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.9043, 1.0000,  ..., 0.9306, 1.0000, 0.9214],\n",
      "        [1.0000, 0.7113, 1.0000,  ..., 0.7140, 1.0000, 0.7115],\n",
      "        [1.0000, 0.9468, 1.0000,  ..., 0.9470, 1.0000, 0.9468],\n",
      "        ...,\n",
      "        [1.0000, 0.9688, 1.0000,  ..., 0.9690, 1.0000, 0.9689],\n",
      "        [1.0000, 0.8092, 1.0000,  ..., 0.8242, 1.0000, 0.8106],\n",
      "        [1.0000, 0.7204, 1.0000,  ..., 0.7708, 1.0000, 0.7330]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6725, 1.0000,  ..., 0.6902, 1.0000, 0.6766],\n",
      "        [1.0000, 0.6972, 1.0000,  ..., 0.7158, 1.0000, 0.6986],\n",
      "        [1.0000, 0.4895, 1.0000,  ..., 0.4929, 1.0000, 0.4896],\n",
      "        ...,\n",
      "        [1.0000, 0.4364, 1.0000,  ..., 0.4386, 1.0000, 0.4365],\n",
      "        [1.0000, 0.7123, 1.0000,  ..., 0.8185, 1.0000, 0.7758],\n",
      "        [1.0000, 0.8088, 1.0000,  ..., 0.8209, 1.0000, 0.8126]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4907, 1.0000,  ..., 0.4948, 1.0000, 0.4910],\n",
      "        [1.0000, 0.5240, 1.0000,  ..., 0.5975, 1.0000, 0.5464],\n",
      "        [1.0000, 0.8748, 1.0000,  ..., 0.8792, 1.0000, 0.8761],\n",
      "        ...,\n",
      "        [1.0000, 0.6103, 1.0000,  ..., 0.6130, 1.0000, 0.6105],\n",
      "        [1.0000, 0.4351, 1.0000,  ..., 0.4637, 1.0000, 0.4572],\n",
      "        [1.0000, 0.9267, 1.0000,  ..., 0.9312, 1.0000, 0.9278]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5062, 1.0000,  ..., 0.5796, 1.0000, 0.5268],\n",
      "        [1.0000, 0.5941, 1.0000,  ..., 0.5963, 1.0000, 0.5942],\n",
      "        [1.0000, 0.4278, 1.0000,  ..., 0.4473, 1.0000, 0.4295],\n",
      "        ...,\n",
      "        [1.0000, 0.4572, 1.0000,  ..., 0.4593, 1.0000, 0.4574],\n",
      "        [1.0000, 0.7653, 1.0000,  ..., 0.7754, 1.0000, 0.7658],\n",
      "        [1.0000, 0.6750, 1.0000,  ..., 0.7574, 1.0000, 0.7097]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 7; bad epochs: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9498, 1.0000,  ..., 0.9839, 1.0000, 0.9734],\n",
      "        [1.0000, 0.4171, 1.0000,  ..., 0.4566, 1.0000, 0.4392],\n",
      "        [1.0000, 0.8495, 1.0000,  ..., 0.9149, 1.0000, 0.8827],\n",
      "        ...,\n",
      "        [1.0000, 0.6200, 1.0000,  ..., 0.6528, 1.0000, 0.6338],\n",
      "        [1.0000, 0.8688, 1.0000,  ..., 0.8715, 1.0000, 0.8689],\n",
      "        [1.0000, 0.7259, 1.0000,  ..., 0.7663, 1.0000, 0.7377]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7561, 1.0000,  ..., 0.7648, 1.0000, 0.7568],\n",
      "        [1.0000, 0.8504, 1.0000,  ..., 0.8626, 1.0000, 0.8531],\n",
      "        [1.0000, 0.5240, 1.0000,  ..., 0.5975, 1.0000, 0.5464],\n",
      "        ...,\n",
      "        [1.0000, 0.7215, 1.0000,  ..., 0.7319, 1.0000, 0.7224],\n",
      "        [1.0000, 0.5729, 1.0000,  ..., 0.6437, 1.0000, 0.5878],\n",
      "        [1.0000, 0.6668, 1.0000,  ..., 0.6712, 1.0000, 0.6672]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4924, 1.0000,  ..., 0.5012, 1.0000, 0.4927],\n",
      "        [1.0000, 0.6317, 1.0000,  ..., 0.6429, 1.0000, 0.6322],\n",
      "        [1.0000, 0.7283, 1.0000,  ..., 0.7691, 1.0000, 0.7497],\n",
      "        ...,\n",
      "        [1.0000, 0.4205, 1.0000,  ..., 0.4425, 1.0000, 0.4224],\n",
      "        [1.0000, 0.8095, 1.0000,  ..., 0.8572, 1.0000, 0.8351],\n",
      "        [1.0000, 0.5598, 1.0000,  ..., 0.5620, 1.0000, 0.5602]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8681, 1.0000,  ..., 0.8792, 1.0000, 0.8694],\n",
      "        [1.0000, 0.4533, 1.0000,  ..., 0.5494, 1.0000, 0.4954],\n",
      "        [1.0000, 0.8188, 1.0000,  ..., 0.8581, 1.0000, 0.8361],\n",
      "        ...,\n",
      "        [1.0000, 0.8862, 1.0000,  ..., 0.8945, 1.0000, 0.8891],\n",
      "        [1.0000, 0.4224, 1.0000,  ..., 0.4294, 1.0000, 0.4226],\n",
      "        [1.0000, 0.6861, 1.0000,  ..., 0.6879, 1.0000, 0.6862]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6335, 1.0000,  ..., 0.6405, 1.0000, 0.6342],\n",
      "        [1.0000, 0.6999, 1.0000,  ..., 0.7268, 1.0000, 0.7031],\n",
      "        [1.0000, 0.7824, 1.0000,  ..., 0.8039, 1.0000, 0.7853],\n",
      "        ...,\n",
      "        [1.0000, 0.6404, 1.0000,  ..., 0.6514, 1.0000, 0.6408],\n",
      "        [1.0000, 0.6386, 1.0000,  ..., 0.7138, 1.0000, 0.6720],\n",
      "        [1.0000, 0.7440, 1.0000,  ..., 0.7488, 1.0000, 0.7441]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5377, 1.0000,  ..., 0.5507, 1.0000, 0.5393],\n",
      "        [1.0000, 0.4948, 1.0000,  ..., 0.5064, 1.0000, 0.4953],\n",
      "        [1.0000, 0.5590, 1.0000,  ..., 0.5662, 1.0000, 0.5591],\n",
      "        ...,\n",
      "        [1.0000, 0.4296, 1.0000,  ..., 0.4399, 1.0000, 0.4299],\n",
      "        [1.0000, 0.4542, 1.0000,  ..., 0.4747, 1.0000, 0.4556],\n",
      "        [1.0000, 0.7636, 1.0000,  ..., 0.7650, 1.0000, 0.7638]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7475, 1.0000,  ..., 0.7509, 1.0000, 0.7478],\n",
      "        [1.0000, 0.7086, 1.0000,  ..., 0.7258, 1.0000, 0.7101],\n",
      "        [1.0000, 0.7448, 1.0000,  ..., 0.7469, 1.0000, 0.7451],\n",
      "        ...,\n",
      "        [1.0000, 0.9262, 1.0000,  ..., 0.9277, 1.0000, 0.9262],\n",
      "        [1.0000, 0.4842, 1.0000,  ..., 0.5187, 1.0000, 0.4882],\n",
      "        [1.0000, 0.4180, 1.0000,  ..., 0.4294, 1.0000, 0.4193]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 102.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 8; bad epochs: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4209, 1.0000,  ..., 0.6181, 1.0000, 0.5458],\n",
      "        [1.0000, 0.7489, 1.0000,  ..., 0.7533, 1.0000, 0.7494],\n",
      "        [1.0000, 0.8003, 1.0000,  ..., 0.8166, 1.0000, 0.8019],\n",
      "        ...,\n",
      "        [1.0000, 0.6865, 1.0000,  ..., 0.7693, 1.0000, 0.7255],\n",
      "        [1.0000, 0.6769, 1.0000,  ..., 0.6818, 1.0000, 0.6771],\n",
      "        [1.0000, 0.5543, 1.0000,  ..., 0.5673, 1.0000, 0.5550]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.4609, 1.0000,  ..., 0.5571, 1.0000, 0.5205],\n",
      "        [1.0000, 0.4817, 1.0000,  ..., 0.4909, 1.0000, 0.4820],\n",
      "        [1.0000, 0.5513, 1.0000,  ..., 0.9154, 1.0000, 0.8511],\n",
      "        ...,\n",
      "        [1.0000, 0.8748, 1.0000,  ..., 0.8792, 1.0000, 0.8761],\n",
      "        [1.0000, 0.7170, 1.0000,  ..., 0.7230, 1.0000, 0.7174],\n",
      "        [1.0000, 0.8489, 1.0000,  ..., 0.8514, 1.0000, 0.8490]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 76.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4412, 1.0000,  ..., 0.4439, 1.0000, 0.4412],\n",
      "        [1.0000, 0.6309, 1.0000,  ..., 0.6312, 1.0000, 0.6309],\n",
      "        [1.0000, 0.7680, 1.0000,  ..., 0.7692, 1.0000, 0.7681],\n",
      "        ...,\n",
      "        [1.0000, 0.4543, 1.0000,  ..., 0.4677, 1.0000, 0.4567],\n",
      "        [1.0000, 0.6854, 1.0000,  ..., 0.7907, 1.0000, 0.7479],\n",
      "        [1.0000, 0.4260, 1.0000,  ..., 0.4731, 1.0000, 0.4326]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8531, 1.0000,  ..., 0.8539, 1.0000, 0.8531],\n",
      "        [1.0000, 0.4305, 1.0000,  ..., 0.4494, 1.0000, 0.4315],\n",
      "        [1.0000, 0.6944, 1.0000,  ..., 0.6987, 1.0000, 0.6948],\n",
      "        ...,\n",
      "        [1.0000, 0.9253, 1.0000,  ..., 0.9264, 1.0000, 0.9254],\n",
      "        [1.0000, 0.5015, 1.0000,  ..., 0.5234, 1.0000, 0.5062],\n",
      "        [1.0000, 0.4990, 1.0000,  ..., 0.5075, 1.0000, 0.4997]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5881, 1.0000,  ..., 0.6128, 1.0000, 0.6007],\n",
      "        [1.0000, 0.8251, 1.0000,  ..., 0.8376, 1.0000, 0.8272],\n",
      "        [1.0000, 0.7195, 1.0000,  ..., 0.7296, 1.0000, 0.7210],\n",
      "        ...,\n",
      "        [1.0000, 0.5894, 1.0000,  ..., 0.6231, 1.0000, 0.6009],\n",
      "        [1.0000, 0.8125, 1.0000,  ..., 0.8785, 1.0000, 0.8421],\n",
      "        [1.0000, 0.6117, 1.0000,  ..., 0.6712, 1.0000, 0.6257]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.5062, 1.0000,  ..., 0.5796, 1.0000, 0.5268],\n",
      "        [1.0000, 0.8080, 1.0000,  ..., 0.8348, 1.0000, 0.8141],\n",
      "        [1.0000, 0.5674, 1.0000,  ..., 0.5908, 1.0000, 0.5727],\n",
      "        ...,\n",
      "        [1.0000, 0.8089, 1.0000,  ..., 0.8117, 1.0000, 0.8090],\n",
      "        [1.0000, 0.4360, 1.0000,  ..., 0.4465, 1.0000, 0.4366],\n",
      "        [1.0000, 0.6941, 1.0000,  ..., 0.7180, 1.0000, 0.6964]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6688, 1.0000,  ..., 0.6919, 1.0000, 0.6714],\n",
      "        [1.0000, 0.7531, 1.0000,  ..., 0.7729, 1.0000, 0.7638],\n",
      "        [1.0000, 0.9008, 1.0000,  ..., 0.9076, 1.0000, 0.9014],\n",
      "        ...,\n",
      "        [1.0000, 0.6905, 1.0000,  ..., 0.7002, 1.0000, 0.6912],\n",
      "        [1.0000, 0.6460, 1.0000,  ..., 0.6554, 1.0000, 0.6469],\n",
      "        [1.0000, 0.8633, 1.0000,  ..., 0.9217, 1.0000, 0.9140]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "-----------------------------------------------------------------\n",
      "epoch: 9; bad epochs: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5975, 1.0000,  ..., 0.6014, 1.0000, 0.5981],\n",
      "        [1.0000, 0.4360, 1.0000,  ..., 0.4465, 1.0000, 0.4366],\n",
      "        [1.0000, 0.5461, 1.0000,  ..., 0.5620, 1.0000, 0.5480],\n",
      "        ...,\n",
      "        [1.0000, 0.4391, 1.0000,  ..., 0.5454, 1.0000, 0.4674],\n",
      "        [1.0000, 0.5674, 1.0000,  ..., 0.5908, 1.0000, 0.5727],\n",
      "        [1.0000, 0.4702, 1.0000,  ..., 0.4774, 1.0000, 0.4704]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.7980, 1.0000,  ..., 0.7995, 1.0000, 0.7980],\n",
      "        [1.0000, 0.5801, 1.0000,  ..., 0.5999, 1.0000, 0.5831],\n",
      "        [1.0000, 0.4244, 1.0000,  ..., 0.4344, 1.0000, 0.4250],\n",
      "        ...,\n",
      "        [1.0000, 0.4351, 1.0000,  ..., 0.4637, 1.0000, 0.4572],\n",
      "        [1.0000, 0.7978, 1.0000,  ..., 0.8021, 1.0000, 0.7979],\n",
      "        [1.0000, 0.4360, 1.0000,  ..., 0.4420, 1.0000, 0.4417]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8170, 1.0000,  ..., 0.8175, 1.0000, 0.8171],\n",
      "        [1.0000, 0.7593, 1.0000,  ..., 0.7690, 1.0000, 0.7604],\n",
      "        [1.0000, 0.6067, 1.0000,  ..., 0.6238, 1.0000, 0.6079],\n",
      "        ...,\n",
      "        [1.0000, 0.6227, 1.0000,  ..., 0.6652, 1.0000, 0.6435],\n",
      "        [1.0000, 0.7201, 1.0000,  ..., 0.7364, 1.0000, 0.7217],\n",
      "        [1.0000, 0.6076, 1.0000,  ..., 0.6139, 1.0000, 0.6077]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8112, 1.0000,  ..., 0.8602, 1.0000, 0.8395],\n",
      "        [1.0000, 0.4237, 1.0000,  ..., 0.4587, 1.0000, 0.4377],\n",
      "        [1.0000, 0.6317, 1.0000,  ..., 0.6429, 1.0000, 0.6322],\n",
      "        ...,\n",
      "        [1.0000, 0.4338, 1.0000,  ..., 0.4386, 1.0000, 0.4340],\n",
      "        [1.0000, 0.5384, 1.0000,  ..., 0.6181, 1.0000, 0.5610],\n",
      "        [1.0000, 0.6877, 1.0000,  ..., 0.7916, 1.0000, 0.7465]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.8531, 1.0000,  ..., 0.8539, 1.0000, 0.8531],\n",
      "        [1.0000, 0.6593, 1.0000,  ..., 0.6662, 1.0000, 0.6600],\n",
      "        [1.0000, 0.6314, 1.0000,  ..., 0.7665, 1.0000, 0.7309],\n",
      "        ...,\n",
      "        [1.0000, 0.7653, 1.0000,  ..., 0.7754, 1.0000, 0.7658],\n",
      "        [1.0000, 0.4392, 1.0000,  ..., 0.4511, 1.0000, 0.4399],\n",
      "        [1.0000, 0.4581, 1.0000,  ..., 0.4840, 1.0000, 0.4623]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([[1.0000, 0.6710, 1.0000,  ..., 0.6765, 1.0000, 0.6711],\n",
      "        [1.0000, 0.7419, 1.0000,  ..., 0.7525, 1.0000, 0.7444],\n",
      "        [1.0000, 0.4816, 1.0000,  ..., 0.5115, 1.0000, 0.4871],\n",
      "        ...,\n",
      "        [1.0000, 0.4768, 1.0000,  ..., 0.4791, 1.0000, 0.4769],\n",
      "        [1.0000, 0.6703, 1.0000,  ..., 0.6830, 1.0000, 0.6771],\n",
      "        [1.0000, 0.6221, 1.0000,  ..., 0.6285, 1.0000, 0.6223]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 84.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7971, 1.0000,  ..., 0.7988, 1.0000, 0.7971],\n",
      "        [1.0000, 0.4202, 1.0000,  ..., 0.4834, 1.0000, 0.4329],\n",
      "        [1.0000, 0.9410, 1.0000,  ..., 0.9439, 1.0000, 0.9414],\n",
      "        ...,\n",
      "        [1.0000, 0.5446, 1.0000,  ..., 0.5660, 1.0000, 0.5460],\n",
      "        [1.0000, 0.9100, 1.0000,  ..., 0.9137, 1.0000, 0.9102],\n",
      "        [1.0000, 0.6074, 1.0000,  ..., 0.6108, 1.0000, 0.6076]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n",
      "loss: nan\n",
      "done, best loss: 1e+16\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_39644/497628390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "loss_progress = np.empty(shape=(0,)) \n",
    "num_bad_epochs = 0\n",
    "\n",
    "for epoch in range(10000): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"epoch: {}; bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    #tqdm shows a progress bar. \n",
    "    for i, sim_E_vox_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred_E_vox, pred_adc, pred_sigma, pred_axr = net(sim_E_vox_batch)\n",
    "        \"\"\"if torch.isnan(pred_E_vox).any():\n",
    "            print(\"evox nan found\")\n",
    "        if torch.isnan(pred_adc).any():\n",
    "            print(\"pred_adc nan found\")\n",
    "        if torch.isnan(pred_axr).any():\n",
    "            print(\"pred_axr nan found\")\n",
    "        if torch.isnan(pred_sigma).any():\n",
    "            print(\"sigpred_sigma nan found\")\"\"\"\n",
    "            \n",
    "        sim_E_vox_batch64 = sim_E_vox_batch.to(torch.float64)\n",
    "        #needed so that loss comparison works\n",
    "        print(sim_E_vox_batch64)\n",
    "        print(pred_E_vox)\n",
    "        loss = criterion(pred_E_vox, sim_E_vox_batch64)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(\"loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"####################### saving good model #######################\")\n",
    "        final_model = net.state_dict()\n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "        loss_progress = np.append(loss_progress, best)\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        loss_progress = np.append(loss_progress, best)\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"done\")\n",
    "\n",
    "net.load_state_dict(final_model)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred_E_vox, final_pred_adc_repeated, final_pred_sigma_repeated, final_pred_axr_repeated = net(torch.from_numpy(sim_E_vox.astype(np.float32)))\n",
    "    # adc sigma and axr will have 8 columns which are all the same\n",
    "\n",
    "final_pred_adc = final_pred_adc_repeated[:, 0]\n",
    "final_pred_sigma = final_pred_sigma_repeated [:, 0]\n",
    "final_pred_axr = final_pred_axr_repeated[:, 0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, len(loss_progress) + 1), loss_progress, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_pred_E_vox_detached = final_pred_E_vox.detach().numpy()\n",
    "\"\"\"Was having numpy pytorch issues, so this line helps fix it a bit.\"\"\"\n",
    "\n",
    "plt.scatter(be, sim_E_vox[0,:], label='simulated')\n",
    "plt.scatter(be, final_pred_E_vox_detached[0,:], label='predicted')\n",
    "plt.legend()\n",
    "\n",
    "# plot scatter plots to analyse correlation of predicted free params against ground truth\n",
    "plt.figure()\n",
    "\n",
    "param_sim = [sim_adc, sim_sigma, sim_axr]\n",
    "param_pred = [final_pred_adc, final_pred_sigma, final_pred_axr]\n",
    "param_name = ['ADC', 'Sigma', 'AXR']\n",
    "\n",
    "rvals = []\n",
    "\n",
    "for i,_ in enumerate(param_sim):\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "    plt.scatter(param_sim[i], param_pred[i], s=2, c='navy')\n",
    "    plt.xlabel(param_name[i] + ' Ground Truth')\n",
    "    plt.ylabel(param_name[i] + ' Prediction')\n",
    "    rvals.append(scipy.stats.pearsonr(np.squeeze(param_sim[i]), np.squeeze(param_pred[i])))\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "print(rvals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530a9f706d6870d645c2a38796218763bd8210a92912827e1b5537d0d478cbf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
