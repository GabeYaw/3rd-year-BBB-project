{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import scipy.stats\n",
    "\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_bvalues\n",
    "from dmipy.core.modeling_framework import MultiCompartmentSphericalMeanModel\n",
    "from dmipy.signal_models import sphere_models, cylinder_models, gaussian_models\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ADC' (Prime)\n",
    "#### I have a feeling this would be much more useful if it took arrays as inputs and outputted an array. But this current method is good for the list comprehension line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adc_prime(adc, sigma, axr, tm):\n",
    "   adc_prime = adc * (1 - sigma* np.exp(-tm*axr))\n",
    "   return adc_prime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sig(adc, sigma, axr, bf, be, tm, acq):\n",
    "    \"\"\"Generate an output signal s based on known inputs for a given voxel\n",
    "Inputs  - adc:      apparent diffusion coefficient [m2/s]\n",
    "        - sigma:    filter efficiency\n",
    "        - axr:      exchange rate [1/s]\n",
    "        - bf:       filter block b-value [m2/s]\n",
    "        - be:       encoding block b-value [m2/s]\n",
    "        - tm:       mixing time [s]\n",
    "\n",
    "Output: - s:        signal (sum of the magnetisations) single value\n",
    "   Based off Elizabeth's code\n",
    "    \"\"\"\n",
    "\n",
    "    if bf == 0 and tm[acq] == min(tm):\n",
    "        tm[acq] = np.inf\n",
    "\n",
    "    #calculate ADC as fnc of mixing time\n",
    "\n",
    "    adc_prime = calc_adc_prime(adc,sigma,axr,tm[acq])\n",
    "\n",
    "    #compute signal\n",
    "    signal = np.exp(-adc_prime*be)\n",
    "\n",
    "    return signal, adc_prime\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 100 # number of voxels to simulate\n",
    "\n",
    "bf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e6   # filter b-values [s/m2]\n",
    "be = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e6       # encoding b-values [s/m2]\n",
    "tm = np.array([20, 20, 20, 20, 200, 200, 400, 400]) * 1e-3  # mixing time [s]\n",
    "\"\"\"^also defined in neural net forward function. See of can avoid duplicate. \"\"\"\n",
    "\n",
    "sim_adc = np.random.uniform(1e-9,1e-10,nvox)                # ADC, simulated [m2/s]\n",
    "sim_sigma = np.random.uniform(0.1,0.9,nvox)                 # sigma, simulated [a.u.]\n",
    "sim_axr = np.random.uniform(1,5,nvox)                       # AXR, simulated [s-1]\n",
    "\n",
    "# simulate signals    \n",
    "sigs_and_adc_prime = np.array([[simulate_sig(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], bf[acq], be[acq], tm, acq) \n",
    "                    for acq in range(np.size(tm))] \n",
    "                    for voxel in range(nvox)])\n",
    "\n",
    "signals = sigs_and_adc_prime[:, :, 0]\n",
    "adc_prime = sigs_and_adc_prime[:, :, 1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # this is the neural network\n",
    "    #defining the init and foward pass functions. \n",
    "\n",
    "    def __init__(self,adc_prime,be,bf,tm,nparams):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.adc_prime = adc_prime\n",
    "        self.be = be\n",
    "        self.bf = bf\n",
    "        self.tm = tm\n",
    "\n",
    "        \"\"\"^Do I need to add adc_prime here?^ or to arguements of init?\n",
    "        Yes, I think it is adc_prime, not signal. See my notes from 28/11 on remarkable.\n",
    "        \"\"\"\n",
    "\n",
    "        #defining the layers that we want. \n",
    "        # 3 layers with no. of be nodes. \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.layers.extend([nn.Linear(len(be), len(be)), nn.PReLU()])\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html\n",
    "        self.encoder = nn.Sequential(*self.layers, nn.Linear(len(be), nparams))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        What is X? snigdha's repo may help \n",
    "        Very unconfident in this as is rn.\n",
    "        \"\"\"\n",
    "\n",
    "        params = torch.nn.functional.softplus(self.encoder(X))\n",
    "        \"\"\" ^unclear of purpose or if needed\"\"\"\n",
    "        #SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive\n",
    "\n",
    "\n",
    "        #params seems be a table containing each variable in a new column. \n",
    "        #unsqueeze adds an additional dimension. \n",
    "        #parameter constraints from Elizabeth matlab \n",
    "        adc = torch.clamp(params[:,0].unsqueeze(1), min=.1e-9, max=3.5e-9) \n",
    "        sigma = torch.clamp(params[:,1].unsqueeze(1), min=0, max=1)\n",
    "        axr = torch.clamp(params[:,2].unsqueeze(1), min=.1, max=20)\n",
    "\n",
    "        tm = np.array([20, 20, 20, 20, 200, 200, 400, 400]) * 1e-3  # mixing time [s]\n",
    "        adc_prime = calc_adc_prime(adc,sigma,axr,tm)\n",
    "        \n",
    "        X = adc_prime\n",
    "\n",
    "        return X, adc, sigma, axr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nparams = 4\n",
    "\n",
    "#initilise network\n",
    "net = Net(adc_prime, be, bf, tm, nparams)\n",
    "\n",
    "#create batch queues for data\n",
    "batch_size = 128\n",
    "#// means divide and round down. \n",
    "num_batches = len(nvox) // batch_size\n",
    "'''Unclear if nvox is correct.'''\n",
    "\n",
    "#import the E_vox array into the dataloader amd convert to float.\n",
    "#drop_last ignores the last batch if it is the wrong size. \n",
    "#num_workers is about performance. \n",
    "'''Unclear what to use instead of E_vox. Maybe adc_prime.'''\n",
    "\n",
    "trainloader = utils.DataLoader(torch.from_numpy(E_vox.astype(np.float32)),\n",
    "                                batch_size = batch_size, \n",
    "                                shuffle = True,\n",
    "                                num_workers = 0, #was 2 previously\n",
    "                                drop_last = True)\n",
    "\n",
    "# loss function and optimizer\n",
    "#choosing which loss function to use. \n",
    "#not sure what the optmizer is\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "# best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "#can increase patience a lot, speed not an issue.\n",
    "patience = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unchanged and untouch from snighda\n",
    "Have not made changes. \n",
    "\"\"\"\n",
    "# train\n",
    "for epoch in range(10000): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"epoch: {}; bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    #tqdm shows a progress bar. \n",
    "    for i, X_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        X_pred, f_ic_pred, f_ees_pred, r_pred, d_ees_pred = net(X_batch)\n",
    "        loss = criterion(X_pred, X_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "      \n",
    "    print(\"loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"####################### saving good model #######################\")\n",
    "        final_model = net.state_dict()\n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"done\")\n",
    "\n",
    "net.load_state_dict(final_model)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    X_real_pred, f_ic, f_ees, r, d_ees = net(torch.from_numpy(E_vox.astype(np.float32)))\n",
    "\n",
    "f_vasc = 1 - f_ic - f_ees\n",
    "\n",
    "f_vasc = f_vasc/(f_ic + f_ees + f_vasc)\n",
    "A = f_vasc\n",
    "normA = A - min(A)\n",
    "f_vasc = 0.2 * (normA/max(normA))\n",
    "f_ic = f_ic/(f_ic + f_ees + f_vasc)\n",
    "f_ees = f_ees/(f_ic + f_ees + f_vasc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 28 2022, 07:24:34) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530a9f706d6870d645c2a38796218763bd8210a92912827e1b5537d0d478cbf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
