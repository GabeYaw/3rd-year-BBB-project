{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import scipy.stats\n",
    "\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_bvalues\n",
    "from dmipy.core.modeling_framework import MultiCompartmentSphericalMeanModel\n",
    "from dmipy.signal_models import sphere_models, cylinder_models, gaussian_models\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ADC' (Prime)\n",
    "#### I have a feeling this would be much more useful if it took arrays as inputs and outputted an array. But this current method is good for the list comprehension line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adc_prime(adc, sigma, axr, tm):\n",
    "   adc_prime = adc * (1 - sigma* np.exp(-tm*axr))\n",
    "   return adc_prime\n",
    "\n",
    "def calc_adc_prime_tensor(adc, sigma, axr, tm):\n",
    "   adc = torch.tensor(adc)\n",
    "   sigma = torch.tensor(sigma)\n",
    "   axr = torch.tensor(axr)\n",
    "\n",
    "   adc_prime = adc * (1 - sigma* torch.exp(-tm*axr))\n",
    "\n",
    "   return adc_prime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sig(adc, sigma, axr, bf, be, tm, acq):\n",
    "    \"\"\"Generate an normalised output signal s based on known inputs for a given voxel\n",
    "Inputs  - adc:      apparent diffusion coefficient [m2/s]\n",
    "        - sigma:    filter efficiency\n",
    "        - axr:      exchange rate [1/s]\n",
    "        - bf:       filter block b-value [m2/s]\n",
    "        - be:       encoding block b-value [m2/s]\n",
    "        - tm:       mixing time [s]\n",
    "\n",
    "Output: - s:        signal (sum of the magnetisations) single value\n",
    "   Based off Elizabeth's code\n",
    "    \"\"\"\n",
    "\n",
    "    if bf == 0 and tm[acq] == min(tm):\n",
    "        tm[acq] = np.inf\n",
    "\n",
    "    #calculate ADC as fnc of mixing time\n",
    "\n",
    "    adc_prime = calc_adc_prime(adc,sigma,axr,tm[acq])\n",
    "\n",
    "    #compute signal\n",
    "    normalised_signal = np.exp(-adc_prime*be)\n",
    "\n",
    "    return normalised_signal, adc_prime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sig_tensor(adc, sigma, axr, bf, be, tm, acq):\n",
    "    \"\"\"Generate an normalised output signal s based on known inputs for a given voxel\n",
    "Inputs  - adc:      apparent diffusion coefficient [m2/s]\n",
    "        - sigma:    filter efficiency\n",
    "        - axr:      exchange rate [1/s]\n",
    "        - bf:       filter block b-value [m2/s]\n",
    "        - be:       encoding block b-value [m2/s]\n",
    "        - tm:       mixing time [s]\n",
    "\n",
    "Output: - s:        signal (sum of the magnetisations) single value\n",
    "   Based off Elizabeth's code\n",
    "    \"\"\"\n",
    "    #maybe move within the forward function\n",
    "    if bf == 0 and tm[acq] == min(tm):\n",
    "        tm[acq] = np.inf\n",
    "\n",
    "    #calculate ADC as fnc of mixing time\n",
    "\n",
    "    adc_prime = calc_adc_prime_tensor(adc,sigma,axr,tm[acq])\n",
    "    \n",
    "    be = torch.tensor(be)  # Convert be to a PyTorch tensor\n",
    "\n",
    "    normalised_signal = torch.exp(-adc_prime * be)\n",
    "\n",
    "    return normalised_signal, adc_prime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnvox1 = 15\\n\\nbf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e6   # filter b-values [s/m2]\\nbe = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e6       # encoding b-values [s/m2]\\ntm = np.array([20, 20, 20, 20, 200, 200, 400, 400]) * 1e-3  # mixing time [s]\\n\\nbe1 = np.tile(be,(nvox1,1))\\nbef = np.tile(bf,(nvox1,1))\\ntm1 = np.tile(tm,(nvox1,1))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt at simulating the signal using arrays\n",
    "\"\"\"\n",
    "nvox1 = 15\n",
    "\n",
    "bf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e6   # filter b-values [s/m2]\n",
    "be = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e6       # encoding b-values [s/m2]\n",
    "tm = np.array([20, 20, 20, 20, 200, 200, 400, 400]) * 1e-3  # mixing time [s]\n",
    "\n",
    "be1 = np.tile(be,(nvox1,1))\n",
    "bef = np.tile(bf,(nvox1,1))\n",
    "tm1 = np.tile(tm,(nvox1,1))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_69683/402710066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m sigs_and_adc_prime = np.array([[simulate_sig(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], bf[acq], be[acq], tm, acq) \n\u001b[1;32m     14\u001b[0m                     for acq in range(np.size(tm))] \n\u001b[0;32m---> 15\u001b[0;31m                     for voxel in range(nvox)])\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#sim_E_vox is the signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_69683/402710066.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m sigs_and_adc_prime = np.array([[simulate_sig(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], bf[acq], be[acq], tm, acq) \n\u001b[1;32m     14\u001b[0m                     for acq in range(np.size(tm))] \n\u001b[0;32m---> 15\u001b[0;31m                     for voxel in range(nvox)])\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#sim_E_vox is the signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_69683/402710066.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# simulate signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m sigs_and_adc_prime = np.array([[simulate_sig(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], bf[acq], be[acq], tm, acq) \n\u001b[0;32m---> 14\u001b[0;31m                     for acq in range(np.size(tm))] \n\u001b[0m\u001b[1;32m     15\u001b[0m                     for voxel in range(nvox)])\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z8/jwxsd17n57lblqk5c_v8fttc0000gn/T/ipykernel_69683/74056071.py\u001b[0m in \u001b[0;36msimulate_sig\u001b[0;34m(adc, sigma, axr, bf, be, tm, acq)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#calculate ADC as fnc of mixing time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "nvox = 1000 # number of voxels to simulate\n",
    "\n",
    "bf = np.array([0, 0, 250, 250, 250, 250, 250, 250]) * 1e-3   # filter b-values [ms/um2]\n",
    "be = np.array([0, 250, 0, 250, 0, 250, 0, 250]) * 1e-3       # encoding b-values [ms/um2]\n",
    "tm = np.array([20, 20, 20, 20, 200, 200, 400, 400])  # mixing time [ms]\n",
    "\n",
    "\n",
    "sim_adc = np.random.uniform(0.1,3.5,nvox)               # ADC, simulated [um2/ms]\n",
    "sim_sigma = np.random.uniform(0,1,nvox)                 # sigma, simulated [a.u.]\n",
    "sim_axr = np.random.uniform(0.1,20,nvox) * 1e-3         # AXR, simulated [ms-1]\n",
    "\n",
    "# simulate signals    \n",
    "sigs_and_adc_prime = np.array([[simulate_sig(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], bf[acq], be[acq], tm, acq) \n",
    "                    for acq in range(np.size(tm))] \n",
    "                    for voxel in range(nvox)])\n",
    "\n",
    "#sim_E_vox is the signal\n",
    "sim_E_vox = sigs_and_adc_prime[:, :, 0]\n",
    "sim_adc_prime = sigs_and_adc_prime[:, :, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting b-value against normalised signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([be[0], be[1]], [sim_E_vox[0,0], sim_E_vox[0,1]], 'bo-')\n",
    "plt.annotate(tm[0], (be[1], sim_E_vox[0, 1]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[2], be[3]], [sim_E_vox[0,2], sim_E_vox[0,3]], 'go-')\n",
    "plt.annotate(tm[2], (be[3], sim_E_vox[0, 3]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[4], be[5]], [sim_E_vox[0,4], sim_E_vox[0,5]], 'ko-')\n",
    "plt.annotate(tm[4], (be[5], sim_E_vox[0, 5]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "plt.plot([be[6], be[7]], [sim_E_vox[0,6], sim_E_vox[0,7]], 'mo-')\n",
    "plt.annotate(tm[6], (be[7], sim_E_vox[0, 7]), textcoords=\"offset points\", xytext=(20,5), ha='center')\n",
    "\n",
    "\n",
    "#plt.title('Scatter plot with 4 lines')\n",
    "plt.xlabel('Encoding block b-value [m2/s]')\n",
    "#are units correct\n",
    "plt.ylabel('Normalised Signal (sum of the magnetisations)')\n",
    "#unit?\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # this is the neural network\n",
    "    #defining the init and foward pass functions. \n",
    "\n",
    "    def __init__(self,be,bf,tm,nparams,batch_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.be = be\n",
    "        self.bf = bf\n",
    "        self.tm = tm\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #defining the layers that we want. \n",
    "        # 3 layers with no. of be nodes. \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.layers.extend([nn.Linear(len(be), len(be)), nn.PReLU()])\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html\n",
    "        self.encoder = nn.Sequential(*self.layers, nn.Linear(len(be), nparams))\n",
    "\n",
    "    def forward(self, E_vox):\n",
    "\n",
    "        params = torch.nn.functional.softplus(self.encoder(E_vox))\n",
    "        #running a forward pass through the network\n",
    "\n",
    "        #SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive\n",
    "        #params contains batch_size x nparams outputs, so each row is adc, sigma and axr.\n",
    "\n",
    "        #unsqueeze adds an additional dimension. \n",
    "        #parameter constraints from Elizabeth matlab \n",
    "        adc = torch.clamp(params[:,0].unsqueeze(1), min=0.1, max=3.5)\n",
    "        \"\"\"I have a feeling adc is worst plot because it is e9 or e10, wheres others are between 0-1 or 20 \n",
    "        Maybe it has do with softplus\n",
    "        \"\"\"\n",
    "        sigma = torch.clamp(params[:,1].unsqueeze(1), min=0, max=1)\n",
    "        axr = torch.clamp(params[:,2].unsqueeze(1), min=.1e-3, max=20e-3)\n",
    "\n",
    "        E_vox = torch.tensor([\n",
    "                [simulate_sig_tensor(sim_adc[voxel], sim_sigma[voxel], sim_axr[voxel], self.bf[acq], self.be[acq], self.tm, acq)[0]\n",
    "                for acq in range(self.tm.size()[0])]\n",
    "                for voxel in range(self.batch_size)\n",
    "                ])\n",
    "\n",
    "\n",
    "        return E_vox, adc, sigma, axr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nparams = 3\n",
    "#because of adc, sigma and axr\n",
    "\n",
    "#converting numpy arrays to pytorch tensors. \n",
    "be = torch.tensor(be)\n",
    "bf = torch.tensor(bf)\n",
    "tm = torch.tensor(tm)\n",
    "batch_size = 128\n",
    "\n",
    "#initilise network\n",
    "net = Net(be, bf, tm, nparams, batch_size)\n",
    "\n",
    "#create batch queues for data\n",
    "#// means divide and round down. \n",
    "num_batches = len(sim_E_vox) // batch_size\n",
    "\n",
    "#import the sim_E_vox array into the dataloader\n",
    "#drop_last ignores the last batch if it is the wrong size. \n",
    "#num_workers is about performance. \n",
    "\n",
    "trainloader = utils.DataLoader(torch.from_numpy(sim_E_vox.astype(np.float32)),\n",
    "                                batch_size = batch_size, \n",
    "                                shuffle = True,\n",
    "                                num_workers = 0, #was 2 previously\n",
    "                                drop_last = True)\n",
    "\n",
    "# loss function and optimizer\n",
    "#choosing which loss function to use. \n",
    "#not sure what the optmizer is\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "# best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "#can increase patience a lot, speed not an issue.\n",
    "patience = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unchanged and untouch from snighda\n",
    "Have not made changes. \n",
    "\"\"\"\n",
    "# train\n",
    "for epoch in range(10000): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"epoch: {}; bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    #tqdm shows a progress bar. \n",
    "    for i, sim_E_vox_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred_E_vox, pred_adc, pred_sigma, pred_axr = net(sim_E_vox_batch)\n",
    "\n",
    "        \"\"\"This is getting messy\"\"\"\n",
    "        #pred_E_vox = torch.tensor(pred_E_vox, dtype=torch.float32, requires_grad=True)\n",
    "        #sim_E_vox_batch = torch.tensor(sim_E_vox_batch, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        pred_E_vox = pred_E_vox.clone().detach().requires_grad_(True).float()\n",
    "        sim_E_vox_batch = sim_E_vox_batch.clone().detach().requires_grad_(True).float()\n",
    "\n",
    "        loss = criterion(pred_E_vox, sim_E_vox_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "      \n",
    "    print(\"loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"####################### saving good model #######################\")\n",
    "        final_model = net.state_dict()\n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"done\")\n",
    "\n",
    "net.load_state_dict(final_model)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred_E_vox, final_pred_adc, final_pred_sigma, final_pred_axr = net(torch.from_numpy(sim_E_vox.astype(np.float32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_E_vox_detached = final_pred_E_vox.detach().numpy()\n",
    "\"\"\"Was having numpy pytorch issues, so this line helps fix it a bit.\"\"\"\n",
    "\n",
    "plt.scatter(be, sim_E_vox[0,:], label='simulated')\n",
    "plt.scatter(be, final_pred_E_vox_detached[0,:], label='predicted')\n",
    "plt.legend()\n",
    "\n",
    "# plot scatter plots to analyse correlation of predicted free params against ground truth\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "param_sim = [sim_adc, sim_sigma, sim_axr]\n",
    "param_pred = [final_pred_adc, final_pred_sigma, final_pred_axr]\n",
    "param_name = ['ADC', 'Sigma', 'AXR']\n",
    "\n",
    "rvals = []\n",
    "\n",
    "for i,_ in enumerate(param_sim):\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "    plt.scatter(param_sim[i], param_pred[i], s=2, c='navy')\n",
    "    plt.xlabel(param_name[i] + ' Ground Truth')\n",
    "    plt.ylabel(param_name[i] + ' Prediction')\n",
    "    rvals.append(scipy.stats.pearsonr(np.squeeze(param_sim[i]), np.squeeze(param_pred[i])))\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "print(rvals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530a9f706d6870d645c2a38796218763bd8210a92912827e1b5537d0d478cbf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
