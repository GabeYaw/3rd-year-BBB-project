{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=OIenNRt2bjg intro to pytorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/project/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import scipy.stats\n",
    "\n",
    "from dmipy.core.acquisition_scheme import acquisition_scheme_from_bvalues\n",
    "from dmipy.core.modeling_framework import MultiCompartmentSphericalMeanModel\n",
    "from dmipy.signal_models import sphere_models, cylinder_models, gaussian_models\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the verdict model compartments. \n",
    "\n",
    "- gradient_strength = np.array([np.sqrt(b_values[i])/(gamma*delta[i]*np.sqrt(Delta[i]-delta[i]/3)) for i,_ in enumerate(b_values)])\n",
    "    -   What does the underscore mean? It might mean an usused variable, but there's only 1 dimension to b_values\n",
    "    - If I did for in enumerate(b_values) then you get back a index and a value, but the i,_ returns just the index and ignores the value. It is a blank unused variable. \n",
    "\n",
    "- What is the purpose of the error function?\n",
    "    - First look at where the astrosticks formula is from, then ask again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# simulate data according to verdict model - there is a function for each of the three compartments\n",
    "# when you do it for bbb-fexi, this will change\n",
    "\n",
    "def sphere(r):\n",
    "    \"\"\" Returns 1x10 array\n",
    "    \"\"\"\n",
    "    # Create 1d array with these 100 values\n",
    "    SPHERE_TRASCENDENTAL_ROOTS = np.r_[\n",
    "        # 0.,\n",
    "        2.081575978, 5.940369990, 9.205840145,\n",
    "        12.40444502, 15.57923641, 18.74264558, 21.89969648,\n",
    "        25.05282528, 28.20336100, 31.35209173, 34.49951492,\n",
    "        37.64596032, 40.79165523, 43.93676147, 47.08139741,\n",
    "        50.22565165, 53.36959180, 56.51327045, 59.65672900,\n",
    "        62.80000055, 65.94311190, 69.08608495, 72.22893775,\n",
    "        75.37168540, 78.51434055, 81.65691380, 84.79941440,\n",
    "        87.94185005, 91.08422750, 94.22655255, 97.36883035,\n",
    "        100.5110653, 103.6532613, 106.7954217, 109.9375497,\n",
    "        113.0796480, 116.2217188, 119.3637645, 122.5057870,\n",
    "        125.6477880, 128.7897690, 131.9317315, 135.0736768,\n",
    "        138.2156061, 141.3575204, 144.4994207, 147.6413080,\n",
    "        150.7831829, 153.9250463, 157.0668989, 160.2087413,\n",
    "        163.3505741, 166.4923978, 169.6342129, 172.7760200,\n",
    "        175.9178194, 179.0596116, 182.2013968, 185.3431756,\n",
    "        188.4849481, 191.6267147, 194.7684757, 197.9102314,\n",
    "        201.0519820, 204.1937277, 207.3354688, 210.4772054,\n",
    "        213.6189378, 216.7606662, 219.9023907, 223.0441114,\n",
    "        226.1858287, 229.3275425, 232.4692530, 235.6109603,\n",
    "        238.7526647, 241.8943662, 245.0360648, 248.1777608,\n",
    "        251.3194542, 254.4611451, 257.6028336, 260.7445198,\n",
    "        263.8862038, 267.0278856, 270.1695654, 273.3112431,\n",
    "        276.4529189, 279.5945929, 282.7362650, 285.8779354,\n",
    "        289.0196041, 292.1612712, 295.3029367, 298.4446006,\n",
    "        301.5862631, 304.7279241, 307.8695837, 311.0112420,\n",
    "        314.1528990\n",
    "    ]\n",
    "\n",
    "    D = 2\n",
    "    gamma = 2.67e2\n",
    "    radius = r\n",
    "\n",
    "    b_values = np.array([1e-6, 0.090, 1e-6, 0.500, 1e-6, 1.5, 1e-6, 2, 1e-6, 3])\n",
    "    Delta = np.array([23.8, 23.8, 23.8, 31.3, 23.8, 43.8, 23.8, 34.3, 23.8, 38.8])\n",
    "    delta = np.array([3.9, 3.9, 3.9, 11.4, 3.9, 23.9, 3.9, 14.4, 3.9, 18.9])\n",
    "\n",
    "    #calculate gradstrength for each b value and (D/d)elta. i is an index. \n",
    "    gradient_strength = np.array([np.sqrt(b_values[i])/(gamma*delta[i]*np.sqrt(Delta[i]-delta[i]/3)) for i,_ in enumerate(b_values)])\n",
    "\n",
    "    # alpha, alpha2 and alpha2d are a 100x1 arrays, the maths is elementwise. \n",
    "    alpha = SPHERE_TRASCENDENTAL_ROOTS / radius\n",
    "    #** means squared, also done elementwise\n",
    "    alpha2 = alpha ** 2\n",
    "    alpha2D = alpha2 * D\n",
    "\n",
    "    #create an array of 1x10. Seems to always be the same. \n",
    "    first_factor = -2 * (gamma * gradient_strength) ** 2 / D\n",
    "    \n",
    "    #create zeros array with lengths as sizes. In this case 100x10\n",
    "    summands = np.zeros((len(SPHERE_TRASCENDENTAL_ROOTS),len(b_values)))\n",
    "    \n",
    "    #create 100x10 array called summands\n",
    "    #equation is summation on page 5 of ssVERDICT paper.\n",
    "    for i,_ in enumerate(delta):\n",
    "        summands[:,i] = (\n",
    "            alpha ** (-4) / (alpha2 * radius ** 2 - 2) *\n",
    "            (\n",
    "                2 * delta[i] - (\n",
    "                    2 +\n",
    "                    np.exp(-alpha2D * (Delta[i] - delta[i])) -\n",
    "                    2 * np.exp(-alpha2D * delta[i]) -\n",
    "                    2 * np.exp(-alpha2D * Delta[i]) +\n",
    "                    np.exp(-alpha2D * (Delta[i] + delta[i]))\n",
    "                ) / (alpha2D)\n",
    "            )\n",
    "        )\n",
    "    #sum all 100 elements in summands, and create E, an array which is 10x1\n",
    "    E = np.exp(\n",
    "        first_factor *\n",
    "        summands.sum()\n",
    "    )\n",
    "    return E\n",
    "\n",
    "def ball(d):\n",
    "    \"\"\" Returns 1x10 array\n",
    "    \"\"\"\n",
    "    bvals = np.array([1e-6, 0.090, 1e-6, 0.500, 1e-6, 1.5, 1e-6, 2, 1e-6, 3])\n",
    "    E_ball = np.exp(-bvals * d)\n",
    "    \n",
    "    #E_ball is a 1x10 array\n",
    "    return E_ball\n",
    "\n",
    "def astrosticks(l):\n",
    "    \"\"\" Returns 1x10 array\n",
    "    \"\"\"\n",
    "    bvals = np.array([1e-6, 0.090, 1e-6, 0.500, 1e-6, 1.5, 1e-6, 2, 1e-6, 3])\n",
    "    lambda_par = l\n",
    "\n",
    "    #make an array filled with ones, same size as bvals (1x10)\n",
    "    E_mean = np.ones_like(bvals)\n",
    "    #erf() is the error function, returns values between -1 to 1. I think it is like normalising a value perhaps.\n",
    "    E_mean = ((np.sqrt(np.pi) * erf(np.sqrt(bvals * lambda_par))) /\n",
    "                (2 * np.sqrt(bvals * lambda_par)))\n",
    "\n",
    "    #E_mean is a 1x10 array\n",
    "    return E_mean \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining number of voxels and verdict parameters\n",
    "- Why is 10 an important and recurring number? Is that the number of b values used?\n",
    "- What does expand_dims() do? It seems to add an extra dimension, but why is that needed?\n",
    "- fvasc seems to be a fraction, not an absolute value. Is this different to fic and fees and why?\n",
    "    - They all eventually become fractions, a few lines later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nvox = 1000 # number of voxels to simulate\n",
    "\n",
    "#These 3 are all 1x nvox arrays. \n",
    "radii = np.random.uniform(0.001,15,nvox) # free parameter - cell radius\n",
    "dees = np.random.uniform(0.5,3,nvox) # free parameter - EES diffusivity\n",
    "lambdapar = np.repeat(2,nvox) # fixed parameter\n",
    "\n",
    "# These are all nvox x 10 arrays. (10 is number of bvals, so I think acquisitions.)\n",
    "E_stick = np.array([astrosticks(l) for l in lambdapar]) \n",
    "E_ball = np.array([ball(d) for d in dees])\n",
    "E_sphere = np.array([sphere(r) for r in radii])\n",
    "\n",
    "# nvox x 1 arrays. \n",
    "fic = np.expand_dims(np.random.uniform(0.001, 0.999, nvox), axis=1) # free parameter - IC volume fraction\n",
    "fees = np.expand_dims(np.random.uniform(0.001, 0.999, nvox), axis=1) # free parameter - EES volume fraction\n",
    "fvasc = 1 - fic - fees # calculate VASC volume fraction\n",
    "fvasc = fvasc/(fic + fees + fvasc)\n",
    "\n",
    "A = fvasc\n",
    "#Smallest value now becomes the new zero, similar to kelvin and celsius. \n",
    "#Done for all elements, so normA is nvox x 1 array\n",
    "normA = A - min(A)\n",
    "fvasc = 0.2 * (normA/max(normA)) # constraining fvasc to be realistic for prostate tissue (under 0.2)\n",
    "\n",
    "#changing into fractions. \n",
    "fic = fic/(fic + fees + fvasc)\n",
    "fees = fees/(fic + fees + fvasc)\n",
    "\n",
    "# These are all nvox x 10 arrays. Adding noise\n",
    "# E_vox is the signal\n",
    "E_vox = fees*E_ball + fic*E_sphere + fvasc*E_stick\n",
    "E_vox_real = E_vox + np.random.normal(scale=0.02, size=np.shape(E_vox)) # adding rician noise, snr = 50\n",
    "E_vox_imag = np.random.normal(scale=0.02, size=np.shape(E_vox))\n",
    "E_vox = np.sqrt(E_vox_real**2 + E_vox_imag**2) # these are the simulated signals\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section for comparing to NLLS fitting.\n",
    "Don't know what it does yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "## this section will be useful when you want to compare it to NLLS fitting, i'm commenting it out for now\n",
    "\n",
    "b_values = np.array([1e-6, 90, 1e-6, 500, 1e-6, 1500, 1e-6, 2000, 1e-6, 3000])\n",
    "bvaluesSI = np.array([i * 1e6 for i in b_values])\n",
    "Delta = np.array([0.0238, 0.0238, 0.0238, 0.0313, 0.0238, 0.0438, 0.0238, 0.0343, 0.0238, 0.0388])\n",
    "delta = np.array([0.0039, 0.0039, 0.0039, 0.0114, 0.0039, 0.0239, 0.0039, 0.0144, 0.0039, 0.0189])\n",
    "gradient_directions = np.loadtxt('./verdict_graddirs.txt', delimiter=',')\n",
    "acq_scheme = acquisition_scheme_from_bvalues(bvaluesSI, gradient_directions, delta, Delta)\n",
    "\n",
    "spheresim = sphere_models.S4SphereGaussianPhaseApproximation(diffusion_constant=2e-9)\n",
    "ballsim = gaussian_models.G1Ball()\n",
    "sticksim = cylinder_models.C1Stick(lambda_par=2e-9)\n",
    "astro = sticksim.spherical_mean(acq_scheme)\n",
    "\n",
    "from dmipy.core.modeling_framework import MultiCompartmentModel\n",
    "verdict_mod = MultiCompartmentModel(models=[spheresim, ballsim, sticksim])\n",
    "verdict_mod.set_parameter_optimization_bounds('G1Ball_1_lambda_iso', [0.5e-9, 3e-9])\n",
    "verdict_mod.set_parameter_optimization_bounds('partial_volume_0', [0.001, 0.999])\n",
    "verdict_mod.set_parameter_optimization_bounds('partial_volume_1', [0.001, 0.999])\n",
    "verdict_mod.set_parameter_optimization_bounds('partial_volume_1', [0.001, 0.199])\n",
    "verdict_mod.set_parameter_optimization_bounds('G1Ball_1_lambda_iso', [0.5e-9, 3e-9])\n",
    "\n",
    "\n",
    "res = verdict_mod.fit(acq_scheme, E_vox)\n",
    "f_ic = res.fitted_parameters['partial_volume_0']\n",
    "f_ees = res.fitted_parameters['partial_volume_1']\n",
    "f_vasc = res.fitted_parameters['partial_volume_2']\n",
    "r = res.fitted_parameters['S4SphereGaussianPhaseApproximation_1_diameter']/2\n",
    "d_ees = res.fitted_parameters['G1Ball_1_lambda_iso']\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network\n",
    "+ What does self.b_values = b_values do?\n",
    "    + assignes b values to the \"self\" version  of b values, ie for that time it was called.\n",
    "+ self.encoder = nn.Sequential(*self.layers, nn.Linear(len(b_values), nparams))\n",
    "    What does this line do? Seems inportant\n",
    "+ Should this be D instead of 2? It is in the first box, so why is this different?\n",
    "    + alpha2D = alpha2 * 2\n",
    "    + NameError: name 'D' is not defined is the error if I put D in. \n",
    "+ What is going on with all the unsqeezes?\n",
    "+ What makes summands be 100 long? \n",
    "    + I get the other 2 dimensions\n",
    "+ What is X?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module): # this is the neural network\n",
    "\n",
    "    #defining the init and foward pass functions. \n",
    "\n",
    "    def __init__(self, b_values, Delta, delta, gradient_strength, nparams):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.b_values = b_values\n",
    "        self.Delta = Delta\n",
    "        self.delta = delta\n",
    "        self.gradient_strength = gradient_strength\n",
    "\n",
    "\n",
    "        #defining the layers that we want. \n",
    "        # 3 layers with 10 (no. of b_values) nodes. See paper.\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(3): # 3 fully connected hidden layers\n",
    "            self.layers.extend([nn.Linear(len(b_values), len(b_values)), nn.PReLU()])\n",
    "            #https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html\n",
    "        self.encoder = nn.Sequential(*self.layers, nn.Linear(len(b_values), nparams))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        constrain f_ic, f_ees, r, d_ees and calculate x.\n",
    "        \"\"\"\n",
    "        params = torch.nn.functional.softplus(self.encoder(X))\n",
    "        #SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.\n",
    "\n",
    "\n",
    "        #params seems be a table containing each variable in a new column. \n",
    "        #unsqueeze adds an additional dimension. \n",
    "        f_ic = torch.clamp(params[:,0].unsqueeze(1), min=0.001, max=0.999) # parameter constraints\n",
    "        f_ees = torch.clamp(params[:,1].unsqueeze(1), min=0.001, max=0.999)\n",
    "        r = torch.clamp(params[:,2].unsqueeze(1), min=0.001, max=14.999)\n",
    "        d_ees = torch.clamp(params[:,3].unsqueeze(1), min=0.5, max=3)\n",
    "\n",
    "        # Create 1d array with these 100 values\n",
    "        SPHERE_TRASCENDENTAL_ROOTS = np.r_[\n",
    "        # 0.,\n",
    "        2.081575978, 5.940369990, 9.205840145,\n",
    "        12.40444502, 15.57923641, 18.74264558, 21.89969648,\n",
    "        25.05282528, 28.20336100, 31.35209173, 34.49951492,\n",
    "        37.64596032, 40.79165523, 43.93676147, 47.08139741,\n",
    "        50.22565165, 53.36959180, 56.51327045, 59.65672900,\n",
    "        62.80000055, 65.94311190, 69.08608495, 72.22893775,\n",
    "        75.37168540, 78.51434055, 81.65691380, 84.79941440,\n",
    "        87.94185005, 91.08422750, 94.22655255, 97.36883035,\n",
    "        100.5110653, 103.6532613, 106.7954217, 109.9375497,\n",
    "        113.0796480, 116.2217188, 119.3637645, 122.5057870,\n",
    "        125.6477880, 128.7897690, 131.9317315, 135.0736768,\n",
    "        138.2156061, 141.3575204, 144.4994207, 147.6413080,\n",
    "        150.7831829, 153.9250463, 157.0668989, 160.2087413,\n",
    "        163.3505741, 166.4923978, 169.6342129, 172.7760200,\n",
    "        175.9178194, 179.0596116, 182.2013968, 185.3431756,\n",
    "        188.4849481, 191.6267147, 194.7684757, 197.9102314,\n",
    "        201.0519820, 204.1937277, 207.3354688, 210.4772054,\n",
    "        213.6189378, 216.7606662, 219.9023907, 223.0441114,\n",
    "        226.1858287, 229.3275425, 232.4692530, 235.6109603,\n",
    "        238.7526647, 241.8943662, 245.0360648, 248.1777608,\n",
    "        251.3194542, 254.4611451, 257.6028336, 260.7445198,\n",
    "        263.8862038, 267.0278856, 270.1695654, 273.3112431,\n",
    "        276.4529189, 279.5945929, 282.7362650, 285.8779354,\n",
    "        289.0196041, 292.1612712, 295.3029367, 298.4446006,\n",
    "        301.5862631, 304.7279241, 307.8695837, 311.0112420,\n",
    "        314.1528990\n",
    "        ]\n",
    "        \n",
    "        # all are 100x1 arrays\n",
    "        alpha = torch.FloatTensor(SPHERE_TRASCENDENTAL_ROOTS) / (r)\n",
    "        alpha2 = alpha ** 2\n",
    "        alpha2D = alpha2 * 2\n",
    "\n",
    "        #add extra dimension\n",
    "        alpha = alpha.unsqueeze(1)\n",
    "        alpha2 = alpha2.unsqueeze(1)\n",
    "        alpha2D = alpha2D.unsqueeze(1)\n",
    "\n",
    "        gamma = 2.675987e2\n",
    "        #elsewhere gradient_strength is 10x1, so I think first factor is also 10x1.\n",
    "        first_factor = -2*(gamma*self.gradient_strength)**2 / 2\n",
    "\n",
    "        #adds 2 dimensions. \n",
    "        delta = self.delta.unsqueeze(0).unsqueeze(2)\n",
    "        Delta = self.Delta.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        #Size of summands is [128, 10, 100]\n",
    "        #128 is batch size, 10 is b_values, not sure what 100 roots.\n",
    "        #create 100x10 array called summands\n",
    "        #equation is summation on page 5 of ssVERDICT paper.\n",
    "        summands = (alpha ** (-4) / (alpha2 * (r.unsqueeze(2))**2 - 2) * (\n",
    "                            2 * delta - (\n",
    "                            2 +\n",
    "                            torch.exp(-alpha2D * (Delta - delta)) -\n",
    "                            2 * torch.exp(-alpha2D * delta) -\n",
    "                            2 * torch.exp(-alpha2D * Delta) +\n",
    "                            torch.exp(-alpha2D * (Delta + delta))\n",
    "                        ) / (alpha2D)\n",
    "                    )\n",
    "                )\n",
    "         \n",
    "        # this is where you will eventually need to tweak the network to be bbb-fexi rather than verdict\n",
    "\n",
    "        #erf() is error function.\n",
    "        #xi,xii,xiii and x are all 128x10. So batch size x 10 b_values most likely.\n",
    "        xi = (1 - f_ic - f_ees) * ((np.sqrt(np.pi) * torch.erf(np.sqrt(self.b_values * 2))) /\n",
    "                (2 * np.sqrt(self.b_values * 2)))\n",
    "        xii = f_ic * torch.exp(torch.FloatTensor(first_factor) * torch.sum(summands, 2))\n",
    "        xiii = f_ees * torch.exp(-self.b_values * d_ees)       \n",
    "        X = xi + xii + xiii\n",
    "\n",
    "        return X, f_ic, f_ees, r, d_ees\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "nparams = 4\n",
    "b_values = torch.FloatTensor([1e-6, 0.090, 1e-6, 0.500, 1e-6, 1.5, 1e-6, 2, 1e-6, 3])\n",
    "Delta = torch.FloatTensor([23.8, 23.8, 23.8, 31.3, 23.8, 43.8, 23.8, 34.3, 23.8, 38.8])\n",
    "delta = torch.FloatTensor([3.9, 3.9, 3.9, 11.4, 3.9, 23.9, 3.9, 14.4, 3.9, 18.9])\n",
    "gamma = 2.67e2\n",
    "#gradient_strength is 10x1.\n",
    "gradient_strength = torch.FloatTensor([np.sqrt(b_values[i])/(gamma*delta[i]*np.sqrt(Delta[i]-delta[i]/3)) for i,_ in enumerate(b_values)])\n",
    "\n",
    "#initilise network\n",
    "net = Net(b_values, Delta, delta, gradient_strength, nparams)\n",
    "\n",
    "#create batch queues for data\n",
    "batch_size = 128\n",
    "#// means divide and round down. \n",
    "num_batches = len(E_vox) // batch_size\n",
    "\n",
    "#import the E_vox array into the dataloader amd convert to float.\n",
    "#drop_last ignores the last batch if it is the wrong size. \n",
    "#num_workers is about performance. \n",
    "trainloader = utils.DataLoader(torch.from_numpy(E_vox.astype(np.float32)),\n",
    "                                batch_size = batch_size, \n",
    "                                shuffle = True,\n",
    "                                num_workers = 0, #was 2 previously\n",
    "                                drop_last = True)\n",
    "\n",
    "# loss function and optimizer\n",
    "#choosing which loss function to use. \n",
    "#not sure what the optmizer is\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "# best loss\n",
    "best = 1e16\n",
    "num_bad_epochs = 0\n",
    "#can increase patients a lot, speed not an issue.\n",
    "patience = 10\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "+ what does net.train() do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "for epoch in range(10000): \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"epoch: {}; bad epochs: {}\".format(epoch, num_bad_epochs))\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    #tqdm shows a progress bar. \n",
    "    for i, X_batch in enumerate(tqdm(trainloader), 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        X_pred, f_ic_pred, f_ees_pred, r_pred, d_ees_pred = net(X_batch)\n",
    "        loss = criterion(X_pred, X_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "      \n",
    "    print(\"loss: {}\".format(running_loss))\n",
    "    # early stopping\n",
    "    if running_loss < best:\n",
    "        print(\"####################### saving good model #######################\")\n",
    "        final_model = net.state_dict()\n",
    "        best = running_loss\n",
    "        num_bad_epochs = 0\n",
    "    else:\n",
    "        num_bad_epochs = num_bad_epochs + 1\n",
    "        if num_bad_epochs == patience:\n",
    "            print(\"done, best loss: {}\".format(best))\n",
    "            break\n",
    "print(\"done\")\n",
    "\n",
    "net.load_state_dict(final_model)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    X_real_pred, f_ic, f_ees, r, d_ees = net(torch.from_numpy(E_vox.astype(np.float32)))\n",
    "\n",
    "f_vasc = 1 - f_ic - f_ees\n",
    "\n",
    "f_vasc = f_vasc/(f_ic + f_ees + f_vasc)\n",
    "A = f_vasc\n",
    "normA = A - min(A)\n",
    "f_vasc = 0.2 * (normA/max(normA))\n",
    "f_ic = f_ic/(f_ic + f_ees + f_vasc)\n",
    "f_ees = f_ees/(f_ic + f_ees + f_vasc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check predicted signal against simulated signal\n",
    "\n",
    "plt.scatter(b_values, E_vox[0,:], label='simulated')\n",
    "plt.scatter(b_values, X_real_pred[0,:], label='predicted')\n",
    "plt.legend()\n",
    "\n",
    "# plot scatter plots to analyse correlation of predicted free params against ground truth\n",
    "plt.figure()\n",
    "\n",
    "param = [fic, fees, fvasc, radii, dees]\n",
    "param_f = [f_ic, f_ees, f_vasc, r, d_ees]\n",
    "param_name = ['fIC', 'fEES', 'fVASC', 'R', 'dEES']\n",
    "rvals = []\n",
    "\n",
    "for i,_ in enumerate(param):\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "    plt.scatter(param[i], param_f[i], s=2, c='navy')\n",
    "    plt.xlabel(param_name[i] + ' Ground Truth')\n",
    "    plt.ylabel(param_name[i] + ' Prediction')\n",
    "    rvals.append(scipy.stats.pearsonr(np.squeeze(param[i]), np.squeeze(param_f[i])))\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "print(rvals)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance calculations\n",
    "+ this section crashes when using 100,000 voxels, although my laptop did switch off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up and see where it breaks. try convert to numpy and use other section.\n",
    "## bias-variance calculations\n",
    "\n",
    "bias_fic = torch.mean(f_ic - fic)\n",
    "bias_fees = torch.mean(f_ees - fees)\n",
    "bias_r = torch.mean(r - radii)\n",
    "bias_dees = torch.mean(d_ees - dees)\n",
    "\n",
    "var_fic = torch.mean((f_ic - torch.mean(f_ic))**2)\n",
    "var_fees = torch.mean((f_ees - torch.mean(f_ees))**2)\n",
    "var_r = torch.mean((r - torch.mean(r))**2)\n",
    "var_dees = torch.mean((d_ees - torch.mean(d_ees))**2)\n",
    "\n",
    "mse_fic = torch.mean((f_ic - fic)**2)\n",
    "mse_fees = torch.mean((f_ees - fees)**2)\n",
    "mse_r = torch.mean((r - radii)**2)\n",
    "mse_dees = torch.mean((d_ees - dees)**2)\n",
    "\n",
    "print(bias_fic, bias_fees, bias_r, bias_dees)\n",
    "print(var_fic, var_fees, var_r, var_dees)\n",
    "print(mse_fic, mse_fees, mse_r, mse_dees)\n",
    "\n",
    "\n",
    "r = r*1e6\n",
    "d_ees = d_ees*1e9\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance calculations using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "bias_fic = np.mean(f_ic - fic)\n",
    "bias_fees = np.mean(f_ees - fees)\n",
    "bias_r = np.mean(r - radii)\n",
    "bias_dees = np.mean(d_ees - dees)\n",
    "\n",
    "var_fic = np.mean((f_ic - np.mean(f_ic))**2)\n",
    "var_fees = np.mean((f_ees - np.mean(f_ees))**2)\n",
    "var_r = np.mean((r - np.mean(r))**2)\n",
    "var_dees = np.mean((d_ees - np.mean(d_ees))**2)\n",
    "\n",
    "mse_fic = np.mean((f_ic - fic)**2)\n",
    "mse_fees = np.mean((f_ees - fees)**2)\n",
    "mse_r = np.mean((r - radii)**2)\n",
    "mse_dees = np.mean((d_ees - dees)**2)\n",
    "\n",
    "print(bias_fic, bias_fees, bias_r, bias_dees)\n",
    "print(var_fic, var_fees, var_r, var_dees)\n",
    "print(mse_fic, mse_fees, mse_r, mse_dees)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "530a9f706d6870d645c2a38796218763bd8210a92912827e1b5537d0d478cbf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
